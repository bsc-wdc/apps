{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Factors\n",
    "\n",
    "## Generates performance metrics from a set of Paraver traces\n",
    "\n",
    "\n",
    "__author__ = \"Michael Wagner\"\n",
    "\n",
    "__copyright__ = \"Copyright 2017, Barcelona Supercomputing Center (BSC)\"\n",
    "\n",
    "__version__ = 0.3.6 + PyCOMPSs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global imports\n",
    "import os\n",
    "import fnmatch\n",
    "import time\n",
    "import numpy\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Externally defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import Trace\n",
    "from helpers import check_installation\n",
    "from helpers import human_readable\n",
    "from helpers import run_command\n",
    "from helpers import save_remove\n",
    "from helpers import get_traces_from_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyCOMPSs imports\n",
    "import pycompss.interactive as ipycompss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the COMPSs runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipycompss.start(graph=True, debug=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycompss.api.task import task\n",
    "from pycompss.api.constraint import constraint\n",
    "from pycompss.api.parameter import *\n",
    "from pycompss.api.api import compss_wait_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contains all raw data entries with a printable name.\n",
    "#This is used to generate and print all raw data, so, if an entry is added, it should be added here, too.\n",
    "raw_data_doc = OrderedDict([('runtime', 'Runtime (us)'), ('runtime_dim', 'Runtime (ideal)'), ('useful_avg', 'Useful duration (average)'), ('useful_max', 'Useful duration (maximum)'), ('useful_tot', 'Useful duration (total)'), ('useful_dim', 'Useful duration (ideal, max)'), ('useful_ins', 'Useful instructions (total)'), ('useful_cyc', 'Useful cycles (total)')])\n",
    "\n",
    "#Contains all model factor entries with a printable name.\n",
    "#This is used to generate and print all model factors, so, if an entry is added, it should be added here, too.\n",
    "mod_factors_doc = OrderedDict([('parallel_eff', 'Parallel efficiency'), ('load_balance', '  Load balance'), ('comm_eff', '  Communication efficiency'), ('serial_eff', '    Serialization efficiency'), ('transfer_eff', '    Transfer efficiency'), ('comp_scale', 'Computation scalability'), ('global_eff', 'Global efficiency'), ('ipc_scale', 'IPC scalability'), ('inst_scale', 'Instruction scalability'), ('freq_scale', 'Frequency scalability'), ('speedup', 'Speedup'), ('ipc', 'Average IPC'), ('freq', 'Average frequency (GHz)')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mod_factors_csv(debug, project):\n",
    "    \"\"\"Reads the model factors table from a csv file.\"\"\"\n",
    "    global mod_factors_doc\n",
    "    delimiter = ';'\n",
    "    file_path = project\n",
    "\n",
    "    # Read csv to list of lines\n",
    "    if os.path.isfile(file_path) and file_path[-4:] == '.csv':\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [line.rstrip('\\n') for line in lines]\n",
    "    else:\n",
    "        raise Exception('==ERROR==', file_path, 'is not a valid csv file.')\n",
    "\n",
    "    # Get the number of processes of the traces\n",
    "    processes = lines[0].split(delimiter)\n",
    "    processes.pop(0)\n",
    "\n",
    "    # Create artificial trace_list and trace_processes\n",
    "    trace_list = []\n",
    "    trace_processes = {}\n",
    "    for process in processes:\n",
    "        trace_list.append(process)\n",
    "        trace_processes[process] = int(process)\n",
    "\n",
    "    # Create empty mod_factors handle\n",
    "    mod_factors = create_mod_factors(trace_list)\n",
    "\n",
    "    # Get mod_factor_doc keys\n",
    "    mod_factors_keys = list(mod_factors_doc.items())\n",
    "\n",
    "    # Iterate over the data lines\n",
    "    for index, line in enumerate(lines[1:len(mod_factors_keys)+1]):\n",
    "        key = mod_factors_keys[index][0]\n",
    "        line = line.split(delimiter)\n",
    "        for index, trace in enumerate(trace_list):\n",
    "            mod_factors[key][trace] = float(line[index+1])\n",
    "\n",
    "    if debug:\n",
    "        print_mod_factors_table(mod_factors, trace_list, trace_processes)\n",
    "\n",
    "    return mod_factors, trace_list, trace_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amdahl(x, x0, f):\n",
    "    \"\"\"Projection function based on amdahl; 2 degrees of freedom: x0, f\"\"\"\n",
    "    return x0 / (f + (1 - f) * x)\n",
    "\n",
    "def pipe(x, x0, f):\n",
    "    \"\"\"Projection function based on pipeline; 2 degrees of freedom: x0, f\"\"\"\n",
    "    return x0 * x / ((1 - f) + f * (2 * x - 1) )\n",
    "\n",
    "def linear(x, x0, f):\n",
    "    \"\"\"Projection function linear; 2 degrees of freedom: x0, a\"\"\"\n",
    "    return x0 + f * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_projection(mod_factors, traces, debug, model, limit, bounds, sigma, gp_out, mpl_out, cfgs_path):\n",
    "    \"\"\"Computes the projection from the gathered model factors and returns the\n",
    "    according dictionary of fitted prediction functions.\"\"\"\n",
    "    \n",
    "    trace_list, trace_processes = get_list_proc(traces)\n",
    "\n",
    "    if debug:\n",
    "        print('==DEBUG== Computing projection of model factors.')\n",
    "\n",
    "    number_traces = len(trace_list)\n",
    "    x_proc = numpy.zeros(number_traces)\n",
    "    y_para = numpy.zeros(number_traces)\n",
    "    y_load = numpy.zeros(number_traces)\n",
    "    y_comm = numpy.zeros(number_traces)\n",
    "    y_comp = numpy.zeros(number_traces)\n",
    "    y_glob = numpy.zeros(number_traces)\n",
    "\n",
    "    # Convert dictionaries to NumPy arrays\n",
    "    for index, trace in enumerate(trace_list):\n",
    "        x_proc[index] = trace_processes[trace]\n",
    "        y_para[index] = mod_factors['parallel_eff'][trace]\n",
    "        y_load[index] = mod_factors['load_balance'][trace]\n",
    "        y_comm[index] = mod_factors['comm_eff'][trace]\n",
    "        y_comp[index] = mod_factors['comp_scale'][trace]\n",
    "        y_glob[index] = mod_factors['global_eff'][trace]\n",
    "\n",
    "    # Select model function\n",
    "    if model == 'amdahl':\n",
    "        _model = amdahl\n",
    "    elif model == 'pipe':\n",
    "        _model = pipe\n",
    "    elif model == 'linear':\n",
    "        _model = linear\n",
    "\n",
    "    # Set limit for projection\n",
    "    if limit:\n",
    "        _limit = str(limit)\n",
    "    else:\n",
    "        _limit = '10000'\n",
    "\n",
    "    # Set boundary for curve fitting parameters: ([x0_min,f_min],[x0_max,f_max])\n",
    "    # For amdahl and pipe f is in [0,1]\n",
    "    if bounds:\n",
    "        _bounds = ([-numpy.inf,0],[numpy.inf,1])\n",
    "    else:\n",
    "        _bounds = ([-numpy.inf,-numpy.inf],[numpy.inf,numpy.inf])\n",
    "\n",
    "    # Set data uncertainty for vector with y-values.\n",
    "    # Smaller values mean higher priority for these y-values.\n",
    "    # Values are compared relatively, not absolute.\n",
    "    if sigma == 'first':\n",
    "        _sigma = numpy.ones(number_traces)\n",
    "        _sigma[0] = 0.1\n",
    "    elif sigma == 'equal':\n",
    "        _sigma = numpy.ones(number_traces)\n",
    "    elif sigma == 'decrease':\n",
    "        _sigma = numpy.linspace(1, 2, number_traces)\n",
    "\n",
    "    # Execute curve fitting, returns optimal parameters array and covariance matrix\n",
    "    # Uses a Levenberg-Marquardt algorithm, i.e. damped least-squares, if no\n",
    "    # bounds are provide; otherwise a Trust Region Reflective algorithm.\n",
    "    # Please note: Both are not true least squares.\n",
    "    # They are greedy methoda and simply run into the nearest local minimum.\n",
    "    # However, this should work fine for this simple 1D optimization.\n",
    "    # Use try to check for SciPy version.\n",
    "    try:\n",
    "        para_opt, para_cov = scipy.optimize.curve_fit(_model, x_proc, y_para, sigma=_sigma, bounds=_bounds)\n",
    "        load_opt, load_cov = scipy.optimize.curve_fit(_model, x_proc, y_load, sigma=_sigma, bounds=_bounds)\n",
    "        comm_opt, comm_cov = scipy.optimize.curve_fit(_model, x_proc, y_comm, sigma=_sigma, bounds=_bounds)\n",
    "        comp_opt, comp_cov = scipy.optimize.curve_fit(_model, x_proc, y_comp, sigma=_sigma, bounds=_bounds)\n",
    "        glob_opt, glob_cov = scipy.optimize.curve_fit(_model, x_proc, y_glob, sigma=_sigma, bounds=_bounds)\n",
    "    except TypeError:\n",
    "        print('==Error== Projection failed! The script requires SciPy 0.17.0 or newer.')\n",
    "        return\n",
    "\n",
    "    # Create the fitting functions for gnuplot; 2 degrees of freedom: x0, f\n",
    "    if _model == amdahl:\n",
    "        gnuplot_fits, matplotlib_fits = fit_amdahl(x_proc, load_opt, comm_opt, comp_opt)\n",
    "    elif _model == pipe:\n",
    "        gnuplot_fits, matplotlib_fits = fit_pipe(x_proc, load_opt, comm_opt, comp_opt)\n",
    "    elif _model == linear:\n",
    "        gnuplot_fits, matplotlib_fits = fit_linear(x_proc, load_opt, comm_opt, comp_opt)\n",
    "\n",
    "    # Select whether para and glob are fitted or multiplied according to model\n",
    "    gnuplot_para_fit = ' '.join(['para( x ) = load( x ) * comm( x ) / 100'])   \n",
    "    gnuplot_glob_fit = ' '.join(['glob( x ) = para( x ) * comp( x ) / 100'])\n",
    "    matplotlib_para_fit = ' '.join(['load( x ) * comm( x ) / 100'])   \n",
    "    matplotlib_glob_fit = ' '.join(['para( x ) * comp( x ) / 100'])\n",
    "\n",
    "    if not os.path.exists(os.path.dirname(gp_out)):\n",
    "        os.makedirs(os.path.dirname(gp_out))\n",
    "    if not os.path.exists(os.path.dirname(mpl_out)):\n",
    "        os.makedirs(os.path.dirname(mpl_out))\n",
    "\n",
    "    points_data = [x_proc, y_para, y_load, y_comm, y_comp, y_glob, number_traces]\n",
    "    create_gnuplot(_limit, gnuplot_para_fit, gnuplot_fits, gnuplot_glob_fit, points_data, cfgs_path, gp_out)\n",
    "    fig = create_matplotlib(_limit, matplotlib_para_fit, matplotlib_fits, matplotlib_glob_fit, points_data, cfgs_path, mpl_out)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_reduce(function, data):\n",
    "    \"\"\" Apply function cumulatively to the items of data,\n",
    "        from left to right in binary tree structure, so as to\n",
    "        reduce the data to a single value.\n",
    "    :param function: function to apply to reduce data\n",
    "    :param data: List of items to be reduced\n",
    "    :return: result of reduce the data to a single value\n",
    "    \"\"\"\n",
    "    from collections import deque\n",
    "    dataNew = data[:]\n",
    "    q = deque(list(range(len(dataNew))))\n",
    "    while len(q):\n",
    "        x = q.popleft()\n",
    "        if len(q):\n",
    "            y = q.popleft()\n",
    "            dataNew[x] = function(dataNew[x], dataNew[y])\n",
    "            q.append(x)\n",
    "        else:\n",
    "            return dataNew[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_reduce_accum(function, data):\n",
    "    \"\"\" Apply function cumulatively to the items of data,\n",
    "        from left to right in binary tree structure, so as to\n",
    "        reduce the data to a single value.\n",
    "    :param function: function to apply to reduce data\n",
    "    :param data: List of items to be reduced\n",
    "    :return: result of reduce the data to a single value\n",
    "    \"\"\"\n",
    "    from collections import deque\n",
    "    dataNew = data[:]\n",
    "    q = deque(list(range(len(dataNew))))\n",
    "    while len(q):\n",
    "        x = q.popleft()\n",
    "        if len(q):\n",
    "            y = q.popleft()\n",
    "            function(dataNew[x], dataNew[y])\n",
    "            q.append(y)\n",
    "        else:\n",
    "            return dataNew[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_data(trace_name):\n",
    "    \"\"\"Creates 2D dictionary of the raw input data and initializes with zero.\n",
    "    The raw_data dictionary has the format: [raw data key][trace].\n",
    "    \"\"\"\n",
    "    global raw_data_doc\n",
    "    raw_data = {}\n",
    "    for key in raw_data_doc:\n",
    "        trace_dict = {}\n",
    "        trace_dict[trace_name] = 0\n",
    "        raw_data[key] = trace_dict\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_raw_data():\n",
    "    \"\"\"Creates 2D dictionary of the raw input data with empty dictionaries for the traces.\n",
    "    The raw_data dictionary has the format: [raw data key]{}.\n",
    "    \"\"\"\n",
    "    global raw_data_doc\n",
    "    raw_data = {}\n",
    "    for key in raw_data_doc:\n",
    "        trace_dict = {}\n",
    "        raw_data[key] = trace_dict\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mod_factors(trace_name):\n",
    "    \"\"\"Creates 2D dictionary of the model factors and initializes with an empty string.\n",
    "    The mod_factors dictionary has the format: [mod factor key][trace].\n",
    "    \"\"\"\n",
    "    global mod_factors_doc\n",
    "    mod_factors = {}\n",
    "    for key in mod_factors_doc:\n",
    "        trace_dict = {}\n",
    "        trace_dict[trace_name] = 0.0\n",
    "        mod_factors[key] = trace_dict\n",
    "    return mod_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_mod_factors():\n",
    "    \"\"\"Creates 2D dictionary of the model factors with empty dictionaries for the traces.\n",
    "    The mod_factors dictionary has the format: [mod factor key]{}.\n",
    "    \"\"\"\n",
    "    global mod_factors_doc\n",
    "    mod_factors = {}\n",
    "    for key in mod_factors_doc:\n",
    "        trace_dict = {}\n",
    "        mod_factors[key] = trace_dict\n",
    "    return mod_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_proc(traces):\n",
    "    \"\"\"Retrieve the traces and their processes\"\"\"\n",
    "    trace_list = traces.keys()\n",
    "    trace_processes = {}\n",
    "    for trace_name in trace_list:\n",
    "        trace_processes[trace_name] = traces[trace_name].get_processes()\n",
    "    return trace_list, trace_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_raw_data_table(raw_data, traces):\n",
    "    \"\"\"Prints the raw data table in human readable form on stdout.\"\"\"\n",
    "    global raw_data_doc\n",
    "    print('Overview of the collected raw data:')\n",
    "    \n",
    "    trace_list, trace_processes = get_list_proc(traces)\n",
    "    \n",
    "    longest_name = len(sorted(raw_data_doc.values(), key=len)[-1])\n",
    "\n",
    "    line = ''.rjust(longest_name)\n",
    "    for trace in trace_list:\n",
    "        line += ' | '\n",
    "        line += str(trace_processes[trace]).rjust(15)\n",
    "    print(line)\n",
    "\n",
    "    print(''.ljust(len(line),'='))\n",
    "    \n",
    "    for data_key in raw_data_doc:\n",
    "        line = raw_data_doc[data_key].ljust(longest_name)\n",
    "        for trace in trace_list:\n",
    "            line += ' | '\n",
    "            line += str(raw_data[data_key][trace]).rjust(15)\n",
    "        print(line)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_raw_data_table_html(raw_data, traces):\n",
    "    \"\"\"Prints the raw data table in html form.\"\"\"\n",
    "    global raw_data_doc\n",
    "    print('Overview of the collected raw data:')\n",
    "    \n",
    "    trace_list, trace_processes = get_list_proc(traces)\n",
    "    \n",
    "    headers = ['Parameter']\n",
    "    data = []\n",
    "\n",
    "    for trace in trace_list:\n",
    "        headers.append(str(trace_processes[trace]))\n",
    "    \n",
    "    for data_key in raw_data_doc:\n",
    "        line = []\n",
    "        line.append(raw_data_doc[data_key])\n",
    "        for trace in trace_list:\n",
    "            line.append(str(raw_data[data_key][trace]))\n",
    "        data.append(line)\n",
    "    \n",
    "    from IPython.display import HTML, display\n",
    "    import tabulate\n",
    "    display(HTML(tabulate.tabulate(data, headers=headers, tablefmt='html', floatfmt=\".2f\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mod_factors_table(mod_factors, traces):\n",
    "    \"\"\"Prints the model factors table in human readable form on stdout.\"\"\"\n",
    "    global mod_factors_doc\n",
    "    print('Overview of the computed model factors:')\n",
    "\n",
    "    longest_name = len(sorted(mod_factors_doc.values(), key=len)[-1])\n",
    "    \n",
    "    trace_list, trace_processes = get_list_proc(traces)\n",
    "    \n",
    "    line = ''.rjust(longest_name)\n",
    "    for trace in trace_list:\n",
    "        line += ' | '\n",
    "        line += str(trace_processes[trace]).rjust(10)\n",
    "    print(line)\n",
    "\n",
    "    print(''.ljust(len(line),'='))\n",
    "\n",
    "    for mod_key in mod_factors_doc:\n",
    "        line = mod_factors_doc[mod_key].ljust(longest_name)\n",
    "        if mod_key in ['speedup','ipc','freq']:\n",
    "            for trace in trace_list:\n",
    "                line += ' | '\n",
    "                try:\n",
    "                    line += ('{0:.2f}'.format(mod_factors[mod_key][trace])).rjust(10)\n",
    "                except ValueError:\n",
    "                    #except NaN\n",
    "                    line += ('{}'.format(mod_factors[mod_key][trace])).rjust(10)\n",
    "        else:\n",
    "            for trace in trace_list:\n",
    "                line += ' | '\n",
    "                try:\n",
    "                    line += ('{0:.2f}%'.format(mod_factors[mod_key][trace])).rjust(10)\n",
    "                except ValueError:\n",
    "                    # except NaN\n",
    "                    line += ('{}'.format(mod_factors[mod_key][trace])).rjust(10)\n",
    "        print(line)\n",
    "        # Print empty line to separate values\n",
    "        if mod_key in ['global_eff','freq_scale']:\n",
    "            line = ''.ljust(longest_name)\n",
    "            for trace in trace_list:\n",
    "                line += ' | '\n",
    "                line += ''.rjust(10)\n",
    "            print(line)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mod_factors_table_html(mod_factors, traces):\n",
    "    \"\"\"Prints the model factors table in html form.\"\"\"\n",
    "    global mod_factors_doc\n",
    "    print('Overview of the computed model factors:')\n",
    "\n",
    "    trace_list, trace_processes = get_list_proc(traces)\n",
    "    \n",
    "    headers = ['Parameter']\n",
    "    data = []\n",
    "    \n",
    "    for trace in trace_list:\n",
    "        headers.append(str(trace_processes[trace]))\n",
    "\n",
    "    for mod_key in mod_factors_doc:\n",
    "        line = []\n",
    "        line.append(mod_factors_doc[mod_key])\n",
    "        for trace in trace_list:\n",
    "            line.append(mod_factors[mod_key][trace])\n",
    "        data.append(line)\n",
    "\n",
    "    from IPython.display import HTML, display\n",
    "    import tabulate\n",
    "    display(HTML(tabulate.tabulate(data, headers=headers, tablefmt='html', floatfmt=\".2f\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mod_factors_csv(mod_factors, raw_data, traces, file_path):\n",
    "    \"\"\"Prints the model factors table in a csv file.\"\"\"\n",
    "    global mod_factors_doc\n",
    "    \n",
    "    trace_list, trace_processes = get_list_proc(traces)\n",
    "\n",
    "    delimiter = ';'\n",
    "    with open(file_path, 'w') as output:\n",
    "        line = 'Number of processes'\n",
    "        for trace in trace_list:\n",
    "            line += delimiter\n",
    "            line += str(trace_processes[trace])\n",
    "        output.write(line + '\\n')\n",
    "\n",
    "        for mod_key in mod_factors_doc:\n",
    "            line = mod_factors_doc[mod_key].replace('  ', '', 2)\n",
    "            for trace in mod_factors[mod_key]:\n",
    "                line += delimiter\n",
    "                try:\n",
    "                    line += '{0:.6f}'.format(mod_factors[mod_key][trace])\n",
    "                except ValueError:\n",
    "                    # except NaN\n",
    "                    line += '{}'.format(mod_factors[mod_key][trace])\n",
    "            output.write(line + '\\n')\n",
    "\n",
    "        output.write('#\\n')\n",
    "\n",
    "        for raw_key in raw_data_doc:\n",
    "            line = '#' + raw_data_doc[raw_key]\n",
    "            for trace in raw_data[raw_key]:\n",
    "                line += delimiter\n",
    "                try:\n",
    "                    line += '{0:.2f}'.format(raw_data[raw_key][trace])\n",
    "                except ValueError:\n",
    "                    # except NaN\n",
    "                    line += '{}'.format(raw_data[raw_key][trace])\n",
    "            output.write(line + '\\n')\n",
    "\n",
    "    print('Model factors written to ' + file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ideal_trace(trace, processes, cfg_dir, debug):\n",
    "    \"\"\"Runs prv2dim and dimemas with ideal configuration for given trace.\"\"\"\n",
    "    trace_dim = trace[:-4] + '.dim'\n",
    "    trace_sim = trace[:-4] + '.sim.prv'\n",
    "    cmd = ['prv2dim', trace, trace_dim]\n",
    "    run_command(cmd, debug)\n",
    "\n",
    "    if os.path.isfile(trace_dim):\n",
    "        if debug:\n",
    "            print('==DEBUG== Created file ' + trace_dim)\n",
    "    else:\n",
    "        print('==Error== ' + trace_dim + 'could not be creaeted.')\n",
    "        return\n",
    "\n",
    "    # Create Dimemas configuration\n",
    "    content = []\n",
    "    with open(os.path.join(cfg_dir, 'dimemas_ideal.cfg')) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    content = [line.replace('REPLACE_BY_NTASKS', str(processes) ) for line in content]\n",
    "    content = [line.replace('REPLACE_BY_COLLECTIVES_PATH', os.path.join(cfg_dir, 'dimemas.collectives')) for line in content]\n",
    "\n",
    "    with open(trace[:-4]+'.dimemas_ideal.cfg', 'w') as f:\n",
    "        f.writelines(content)\n",
    "\n",
    "    cmd = ['Dimemas', '-S', '32k', '--dim', trace_dim, '-p', trace_sim, trace[:-4]+'.dimemas_ideal.cfg']\n",
    "    run_command(cmd, debug)\n",
    "\n",
    "    os.remove(trace_dim)\n",
    "    os.remove(trace[:-4]+'.dimemas_ideal.cfg')\n",
    "\n",
    "    if os.path.isfile(trace_sim):\n",
    "        if debug:\n",
    "            print('==DEBUG== Created file ' + trace_sim)\n",
    "        return trace_sim\n",
    "    else:\n",
    "        print('==Error== ' + trace_sim + ' could not be creaeted.')\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(returns=str)\n",
    "def get_scaling_type(raw_data, traces, scaling, debug):\n",
    "    \"\"\"Guess the scaling type (weak/strong) based on the useful instructions.\n",
    "    Computes the normalized instruction ratio for all measurements, whereas the\n",
    "    normalized instruction ratio is (instructions ratio / process ratio) with\n",
    "    the smallest run as reference. For exact weak scaling the normalized ratio\n",
    "    should be exactly 1 and for exact strong scaling it should be close to zero\n",
    "    with an upper bound of 0.5. The eps value defines the threshold to be\n",
    "    considered weak scaling and should give enough buffer to safely handle\n",
    "    non-ideal scaling.\n",
    "    \"\"\"\n",
    "    eps = 0.9\n",
    "    normalized_inst_ratio = 0\n",
    "    \n",
    "    trace_list = traces.keys()\n",
    "    first_trace_processes = None\n",
    "    trace_processes = []\n",
    "    for trace_name in trace_list:\n",
    "        trace_processes = traces[trace_name].get_processes()\n",
    "        if first_trace_processes is None:\n",
    "            first_trace_processes = trace_processes\n",
    "\n",
    "    # Check if there is only one trace.\n",
    "    if len(trace_list) == 1:\n",
    "        return 'strong'\n",
    "\n",
    "    for trace in trace_list:\n",
    "        inst_ratio = float(raw_data['useful_ins'][trace]) / float(raw_data['useful_ins'][trace_list[0]])\n",
    "        proc_ratio = float(trace_processes) / float(first_trace_processes)\n",
    "        normalized_inst_ratio += inst_ratio / proc_ratio\n",
    "\n",
    "    # Get the average inst increase. Ignore ratio of first trace 1.0)\n",
    "    normalized_inst_ratio = (normalized_inst_ratio - 1) / (len(trace_list) - 1)\n",
    "\n",
    "    scaling_computed = ''\n",
    "\n",
    "    if normalized_inst_ratio > eps:\n",
    "        scaling_computed = 'weak'\n",
    "    else:\n",
    "        scaling_computed = 'strong'\n",
    "\n",
    "    if scaling == 'auto':\n",
    "        if debug:\n",
    "            print('==DEBUG== Detected ' + scaling_computed + ' scaling.')\n",
    "            print('')\n",
    "        return scaling_computed\n",
    "\n",
    "    if scaling == 'weak':\n",
    "        if scaling_computed == 'strong':\n",
    "            print('==Warning== Scaling set to weak scaling but detected strong scaling.')\n",
    "            print('')\n",
    "        return 'weak'\n",
    "\n",
    "    if scaling == 'strong':\n",
    "        if scaling_computed == 'weak':\n",
    "            print('==Warning== Scaling set to strong scaling but detected weak scaling.')\n",
    "            print('')\n",
    "        return 'strong'\n",
    "\n",
    "    raise Exception('==Error== reached undefined control flow state.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(trace=FILE_IN, timings=FILE_IN, runtime=FILE_IN, cycles=FILE_IN, inst=FILE_IN, dimemas_cfgs=FILE_IN, dimemas_collectives=FILE_IN, returns=dict)\n",
    "def gather_raw_data(trace, timings, runtime, cycles, inst, dimemas_cfgs, dimemas_collectives, trace_processes, cfgs_path, debug):\n",
    "    \"\"\"Gathers all raw data needed to generate the model factors. Return raw\n",
    "    data in a 2D dictionary <data type><list of values for each trace>\"\"\"\n",
    "    trace_name = os.path.basename(trace)\n",
    "    raw_data = create_raw_data(trace_name)\n",
    "\n",
    "    cfgs = {}\n",
    "    cfgs['root_dir']      = cfgs_path\n",
    "    cfgs['timings']       = os.path.join(cfgs['root_dir'], 'timings.cfg')\n",
    "    cfgs['runtime']       = os.path.join(cfgs['root_dir'], 'runtime.cfg')\n",
    "    cfgs['cycles']        = os.path.join(cfgs['root_dir'], 'cycles.cfg')\n",
    "    cfgs['instructions']  = os.path.join(cfgs['root_dir'], 'instructions.cfg')\n",
    "\n",
    "    # Main loop over all traces\n",
    "    time_tot = time.time()\n",
    "\n",
    "    line = 'Analyzing ' + os.path.basename(trace)\n",
    "    line += ' (' + str(trace_processes) + ' processes'\n",
    "    line += ', ' + human_readable( os.path.getsize( trace ) ) + ')'\n",
    "    print(line)\n",
    "\n",
    "    # Create simulated ideal trace with Dimemas\n",
    "    time_dim = time.time()\n",
    "    trace_sim = create_ideal_trace(trace, trace_processes, cfgs['root_dir'], debug)\n",
    "    time_dim = time.time() - time_dim\n",
    "    if not trace_sim == '':\n",
    "        print('Successfully created simulated trace with Dimemas in {0:.1f} seconds.'.format(time_dim))\n",
    "    else:\n",
    "        print('Failed to create simulated trace with Dimemas.')\n",
    "\n",
    "    # Run paramedir for the original and simulated trace\n",
    "    time_pmd = time.time()\n",
    "    cmd_normal = ['paramedir', trace]\n",
    "    cmd_normal.extend([cfgs['timings'],      trace[:-4] + '.timings.stats'])\n",
    "    cmd_normal.extend([cfgs['runtime'],      trace[:-4] + '.runtime.stats'])\n",
    "    cmd_normal.extend([cfgs['cycles'],       trace[:-4] + '.cycles.stats'])\n",
    "    cmd_normal.extend([cfgs['instructions'], trace[:-4] + '.instructions.stats'])\n",
    "\n",
    "    cmd_ideal = ['paramedir', trace_sim]\n",
    "    cmd_ideal.extend([cfgs['timings'],       trace_sim[:-4] + '.timings.stats'])\n",
    "    cmd_ideal.extend([cfgs['runtime'],       trace_sim[:-4] + '.runtime.stats'])\n",
    "\n",
    "    run_command(cmd_normal, debug)\n",
    "    if not trace_sim == '':\n",
    "        run_command(cmd_ideal, debug)\n",
    "\n",
    "    time_pmd = time.time() - time_pmd\n",
    "\n",
    "    error_timing = 0;\n",
    "    error_counters = 0;\n",
    "    error_ideal = 0;\n",
    "\n",
    "    # Check if all files are created\n",
    "    if not os.path.exists(trace[:-4] + '.timings.stats') or \\\n",
    "       not os.path.exists(trace[:-4] + '.runtime.stats'):\n",
    "        print('==ERROR== Failed to compute timing information with paramedir.')\n",
    "        error_timing = 1\n",
    "\n",
    "    if not os.path.exists(trace[:-4] + '.cycles.stats') or \\\n",
    "       not os.path.exists(trace[:-4] + '.instructions.stats'):\n",
    "        print('==ERROR== Failed to compute counter information with paramedir.')\n",
    "        error_counters = 1\n",
    "\n",
    "    if not os.path.exists(trace_sim[:-4] + '.timings.stats') or \\\n",
    "       not os.path.exists(trace_sim[:-4] + '.runtime.stats'):\n",
    "        print('==ERROR== Failed to compute timing information with paramedir.')\n",
    "        error_ideal = 1\n",
    "        trace_sim = ''\n",
    "\n",
    "    if error_timing or error_counters or error_ideal:\n",
    "        print('Failed to analyze trace with paramedir in {0:.1f} seconds.'.format(time_pmd))\n",
    "    else:\n",
    "        print('Successfully analyzed trace with paramedir in {0:.1f} seconds.'.format(time_pmd))\n",
    "\n",
    "\n",
    "    # Parse the paramedir output files\n",
    "    time_prs = time.time()\n",
    "\n",
    "    # Get total, average, and maximum useful duration\n",
    "    if os.path.exists(trace[:-4] + '.timings.stats'):\n",
    "        content = []\n",
    "        with open(trace[:-4] + '.timings.stats') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        for line in content:\n",
    "            if line.split():\n",
    "                if line.split()[0] == 'Total':\n",
    "                    raw_data['useful_tot'][trace_name] = float(line.split()[1])\n",
    "                if line.split()[0] == 'Average':\n",
    "                    raw_data['useful_avg'][trace_name] = float(line.split()[1])\n",
    "                if line.split()[0] == 'Maximum':\n",
    "                    raw_data['useful_max'][trace_name] = float(line.split()[1])\n",
    "    else:\n",
    "        raw_data['useful_tot'][trace_name] = 'NaN'\n",
    "        raw_data['useful_avg'][trace_name] = 'NaN'\n",
    "        raw_data['useful_max'][trace_name] = 'NaN'\n",
    "\n",
    "    # Get runtime\n",
    "    if os.path.exists(trace[:-4] + '.runtime.stats'):\n",
    "        content = []\n",
    "        with open(trace[:-4] + '.runtime.stats') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        for line in content:\n",
    "            if line.split():\n",
    "                if line.split()[0] == 'Average':\n",
    "                    raw_data['runtime'][trace_name] = float(line.split()[1])\n",
    "    else:\n",
    "        raw_data['runtime'][trace_name] = 'NaN'\n",
    "\n",
    "    # Get useful cycles\n",
    "    if os.path.exists(trace[:-4] + '.cycles.stats'):\n",
    "        content = []\n",
    "        with open(trace[:-4] + '.cycles.stats') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        for line in content:\n",
    "            if line.split():\n",
    "                if line.split()[0] == 'Total':\n",
    "                    raw_data['useful_cyc'][trace_name] = int(float(line.split()[1]))\n",
    "    else:\n",
    "        raw_data['useful_cyc'][trace_name] = 'NaN'\n",
    "\n",
    "    # Get useful instructions\n",
    "    if os.path.exists(trace[:-4] + '.instructions.stats'):\n",
    "        content = []\n",
    "        with open(trace[:-4] + '.instructions.stats') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        for line in content:\n",
    "            if line.split():\n",
    "                if line.split()[0] == 'Total':\n",
    "                    raw_data['useful_ins'][trace_name] = int(float(line.split()[1]))\n",
    "    else:\n",
    "        raw_data['useful_ins'][trace_name] ='NaN'\n",
    "\n",
    "    # Get maximum useful duration for simulated trace\n",
    "    if os.path.exists(trace_sim[:-4] + '.timings.stats'):\n",
    "        content = []\n",
    "        with open(trace_sim[:-4] + '.timings.stats') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        for line in content:\n",
    "            if line.split():\n",
    "                if line.split()[0] == 'Maximum':\n",
    "                    raw_data['useful_dim'][trace_name] = float(line.split()[1])\n",
    "    else:\n",
    "        raw_data['useful_dim'][trace_name] = 'NaN'\n",
    "\n",
    "    # Get runtime for simulated trace\n",
    "    if os.path.exists(trace_sim[:-4] + '.runtime.stats'):\n",
    "        content = []\n",
    "        with open(trace_sim[:-4] + '.runtime.stats') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        for line in content:\n",
    "            if line.split():\n",
    "                if line.split()[0] == 'Average':\n",
    "                    raw_data['runtime_dim'][trace_name] = float(line.split()[1])\n",
    "    else:\n",
    "        raw_data['runtime_dim'][trace_name] = 'NaN'\n",
    "\n",
    "    # Remove paramedir output files\n",
    "    save_remove(trace[:-4] + '.timings.stats', debug)\n",
    "    save_remove(trace[:-4] + '.runtime.stats', debug)\n",
    "    save_remove(trace[:-4] + '.cycles.stats', debug)\n",
    "    save_remove(trace[:-4] + '.instructions.stats', debug)\n",
    "    save_remove(trace_sim[:-4] + '.timings.stats', debug)\n",
    "    save_remove(trace_sim[:-4] + '.runtime.stats', debug)\n",
    "    time_prs = time.time() - time_prs\n",
    "\n",
    "    time_tot = time.time() - time_tot\n",
    "    print('Finished successfully in {0:.1f} seconds.'.format(time_tot))\n",
    "    print('')\n",
    "\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(trace=FILE_IN, returns=dict)\n",
    "def compute_model_factors(raw_data, trace, trace_processes, first_trace, first_trace_processes, scaling, debug):\n",
    "    \"\"\"Computes the model factors from the gathered raw data and returns the\n",
    "    according dictionary of model factors.\"\"\"\n",
    "    trace_name = os.path.basename(trace)\n",
    "    mod_factors = create_mod_factors(trace_name)\n",
    "    \n",
    "    proc_ratio = float(trace_processes) / float(first_trace_processes)\n",
    "    \n",
    "    # Basic efficiency factors\n",
    "    try:  # except NaN\n",
    "        mod_factors['load_balance'][trace_name] = raw_data['useful_avg'][trace_name] / raw_data['useful_max'][trace_name] * 100.0\n",
    "    except:\n",
    "        mod_factors['load_balance'][trace_name] = 'NaN'\n",
    "\n",
    "    try:  # except NaN\n",
    "        mod_factors['comm_eff'][trace_name] = raw_data['useful_max'][trace_name] / raw_data['runtime'][trace_name] * 100.0\n",
    "    except:\n",
    "        mod_factors['comm_eff'][trace_name] = 'NaN'\n",
    "\n",
    "    try:  # except NaN\n",
    "        mod_factors['serial_eff'][trace_name] = raw_data['useful_dim'][trace_name] / raw_data['runtime_dim'][trace_name] * 100.0\n",
    "    except:\n",
    "        mod_factors['serial_eff'][trace_name] = 'NaN'\n",
    "\n",
    "    try:  # except NaN\n",
    "        mod_factors['transfer_eff'][trace_name] = mod_factors['comm_eff'][trace_name] / mod_factors['serial_eff'][trace_name] * 100.0\n",
    "    except:\n",
    "        mod_factors['transfer_eff'][trace_name] = 'NaN'\n",
    "\n",
    "    try:  # except NaN\n",
    "        mod_factors['parallel_eff'][trace_name] = mod_factors['load_balance'][trace_name] * mod_factors['comm_eff'][trace_name] / 100.0\n",
    "    except:\n",
    "        mod_factors['parallel_eff'][trace_name] = 'NaN'\n",
    "\n",
    "    try:  # except NaN\n",
    "        if scaling == 'strong':\n",
    "            mod_factors['comp_scale'][trace_name] = raw_data['useful_tot'][first_trace] / raw_data['useful_tot'][trace_name] * 100.0\n",
    "        else:\n",
    "            mod_factors['comp_scale'][trace_name] = raw_data['useful_tot'][first_trace] / raw_data['useful_tot'][trace_name] * proc_ratio * 100.0\n",
    "    except:\n",
    "        mod_factors['comp_scale'][trace_name] = 'NaN'\n",
    "\n",
    "    try:  # except NaN\n",
    "        mod_factors['global_eff'][trace_name] = mod_factors['parallel_eff'][trace_name] * mod_factors['comp_scale'][trace_name] / 100.0\n",
    "    except:\n",
    "        mod_factors['global_eff'][trace_name] = 'NaN'\n",
    "\n",
    "    # Basic scalability factors\n",
    "    try:  # except NaN\n",
    "        mod_factors['ipc'][trace_name] = float(raw_data['useful_ins'][trace_name]) / float(raw_data['useful_cyc'][trace_name])\n",
    "    except:\n",
    "        mod_factors['ipc'][trace_name] = 'NaN'\n",
    "    try:  # except NaN\n",
    "        ipc_first_trace = float(raw_data['useful_ins'][first_trace]) / float(raw_data['useful_cyc'][first_trace])\n",
    "        mod_factors['ipc_scale'][trace_name] = mod_factors['ipc'][trace_name] / ipc_first_trace * 100.0\n",
    "        # Do not reuse mod_factors for the first_trace to avoid INOUT\n",
    "        # mod_factors['ipc_scale'][trace_name] = mod_factors['ipc'][trace_name] / mod_factors['ipc'][first_trace] * 100.0\n",
    "    except Exception as e:\n",
    "        mod_factors['ipc_scale'][trace_name] = 'NaN'\n",
    "    try:  # except NaN\n",
    "        mod_factors['freq'][trace_name] = float(raw_data['useful_cyc'][trace_name]) / float(raw_data['useful_tot'][trace_name]) / 1000\n",
    "    except:\n",
    "        mod_factors['freq'][trace_name] = 'NaN'\n",
    "    try:  # except NaN\n",
    "        freq_first_trace = float(raw_data['useful_cyc'][first_trace]) / float(raw_data['useful_tot'][first_trace]) / 1000\n",
    "        mod_factors['freq_scale'][trace_name] = mod_factors['freq'][trace_name] / freq_first_trace * 100.0\n",
    "        # Do not reuse mod_factors for the first_trace to avoid INOUT\n",
    "        # mod_factors['freq_scale'][trace_name] = mod_factors['freq'][trace_name] / mod_factors['freq'][first_trace] * 100.0\n",
    "    except Exception as e:\n",
    "        mod_factors['freq_scale'][trace_name] = 'NaN'\n",
    "    try:  # except NaN\n",
    "        if scaling == 'strong':\n",
    "            mod_factors['inst_scale'][trace_name] = float(raw_data['useful_ins'][first_trace]) / float(raw_data['useful_ins'][trace_name]) * 100.0\n",
    "        else:\n",
    "            mod_factors['inst_scale'][trace_name] = float(raw_data['useful_ins'][first_trace]) / float(raw_data['useful_ins'][trace_name]) * proc_ratio * 100.0\n",
    "    except:\n",
    "        mod_factors['inst_scale'][trace_name] = 'NaN'\n",
    "    try:  # except NaN\n",
    "        if scaling == 'strong':\n",
    "            mod_factors['speedup'][trace_name] = raw_data['runtime'][first_trace] / raw_data['runtime'][trace_name]\n",
    "        else:\n",
    "            mod_factors['speedup'][trace_name] = raw_data['runtime'][first_trace] / raw_data['runtime'][trace_name] * proc_ratio\n",
    "    except:\n",
    "        mod_factors['speedup'][trace_name] = 'NaN'\n",
    "\n",
    "    return mod_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(returns=(list, list))\n",
    "def fit_amdahl(x_proc, load_opt, comm_opt, comp_opt):\n",
    "    \"\"\"Get amdahl fit function for gnuplot and matplotlib.\"\"\"\n",
    "    gnuplot_fits = []\n",
    "    load_fit = ' '.join(['load( x ) = ( x >',str(x_proc[0]),') ?',str(load_opt[0]),'/ (',str(load_opt[1]),'+ ( 1 -',str(load_opt[1]),') * x ) : 1/0'])\n",
    "    comm_fit = ' '.join(['comm( x ) = ( x >',str(x_proc[0]),') ?',str(comm_opt[0]),'/ (',str(comm_opt[1]),'+ ( 1 -',str(comm_opt[1]),') * x ) : 1/0'])\n",
    "    comp_fit = ' '.join(['comp( x ) = ( x >',str(x_proc[0]),') ?',str(comp_opt[0]),'/ (',str(comp_opt[1]),'+ ( 1 -',str(comp_opt[1]),') * x ) : 1/0'])\n",
    "    gnuplot_fits.append(load_fit)\n",
    "    gnuplot_fits.append(comm_fit)\n",
    "    gnuplot_fits.append(comp_fit)\n",
    "    matplotlib_fits = []\n",
    "    load_fit = ' '.join([str(load_opt[0]),'/ (',str(load_opt[1]),'+ ( 1 -',str(load_opt[1]),') * x )', 'if ( x >=',str(x_proc[0]),') else 1/0'])\n",
    "    comm_fit = ' '.join([str(comm_opt[0]),'/ (',str(comm_opt[1]),'+ ( 1 -',str(comm_opt[1]),') * x )', 'if ( x >=',str(x_proc[0]),') else 1/0'])\n",
    "    comp_fit = ' '.join([str(comp_opt[0]),'/ (',str(comp_opt[1]),'+ ( 1 -',str(comp_opt[1]),') * x )', 'if ( x >=',str(x_proc[0]),') else 1/0'])\n",
    "    matplotlib_fits.append(load_fit)\n",
    "    matplotlib_fits.append(comm_fit)\n",
    "    matplotlib_fits.append(comp_fit)\n",
    "    return gnuplot_fits, matplotlib_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(returns=(list, list))\n",
    "def fit_pipe(x_proc, load_opt, comm_opt, comp_opt):\n",
    "    \"\"\"Get pipe fit function for gnuplot and matplotlib.\"\"\"\n",
    "    gnuplot_fits = []\n",
    "    load_fit = ' '.join(['load( x ) = ( x >', str(x_proc[0]),') ?', str(load_opt[0]),'* x / ( ( 1 -', str(load_opt[1]),') +', str(load_opt[1]),'* ( 2 * x - 1 ) ) : 1/0'])\n",
    "    comm_fit = ' '.join(['comm( x ) = ( x >', str(x_proc[0]),') ?', str(comm_opt[0]),'* x / ( ( 1 -', str(comm_opt[1]),') +', str(comm_opt[1]),'* ( 2 * x - 1 ) ) : 1/0'])\n",
    "    comp_fit = ' '.join(['comp( x ) = ( x >', str(x_proc[0]),') ?', str(comp_opt[0]),'* x / ( ( 1 -', str(comp_opt[1]),') +', str(comp_opt[1]),'* ( 2 * x - 1 ) ) : 1/0'])\n",
    "    gnuplot_fits.append(load_fit)\n",
    "    gnuplot_fits.append(comm_fit)\n",
    "    gnuplot_fits.append(comp_fit)\n",
    "    matplotlib_fits = []\n",
    "    load_fit = ' '.join([str(load_opt[0]),'* x / ( ( 1 -', str(load_opt[1]),') +', str(load_opt[1]),'* ( 2 * x - 1 ) )', 'if ( x >=', str(x_proc[0]),') else 1/0'])\n",
    "    comm_fit = ' '.join([str(comm_opt[0]),'* x / ( ( 1 -', str(comm_opt[1]),') +', str(comm_opt[1]),'* ( 2 * x - 1 ) )', 'if ( x >=', str(x_proc[0]),') else 1/0'])\n",
    "    comp_fit = ' '.join([str(comp_opt[0]),'* x / ( ( 1 -', str(comp_opt[1]),') +', str(comp_opt[1]),'* ( 2 * x - 1 ) )', 'if ( x >=', str(x_proc[0]),') else 1/0'])\n",
    "    matplotlib_fits.append(load_fit)\n",
    "    matplotlib_fits.append(comm_fit)\n",
    "    matplotlib_fits.append(comp_fit)\n",
    "    return gnuplot_fits, matplotlib_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(returns=(list, list))\n",
    "def fit_linear(x_proc, load_opt, comm_opt, comp_opt):\n",
    "    \"\"\"Get linear fit function for gnuplot and matplotlib.\"\"\"\n",
    "    gnuplot_fits = []\n",
    "    load_fit = ' '.join(['load( x ) = ( x >', str(x_proc[0]), ') ?', str(load_opt[0]), '+ x *', str(load_opt[1]), ': 1/0'])\n",
    "    comm_fit = ' '.join(['comm( x ) = ( x >', str(x_proc[0]), ') ?', str(comm_opt[0]), '+ x *', str(comm_opt[1]), ': 1/0'])\n",
    "    comp_fit = ' '.join(['comp( x ) = ( x >', str(x_proc[0]), ') ?', str(comp_opt[0]), '+ x *', str(comp_opt[1]), ': 1/0'])\n",
    "    gnuplot_fits.append(load_fit)\n",
    "    gnuplot_fits.append(comm_fit)\n",
    "    gnuplot_fits.append(comp_fit)\n",
    "    matplotlib_fits = []\n",
    "    load_fit = ' '.join([str(load_opt[0]), '+ x *', str(load_opt[1]), 'if ( x >=', str(x_proc[0]), ') else 1/0'])\n",
    "    comm_fit = ' '.join([str(comm_opt[0]), '+ x *', str(comm_opt[1]), 'if ( x >=', str(x_proc[0]), ') else 1/0'])\n",
    "    comp_fit = ' '.join([str(comp_opt[0]), '+ x *', str(comp_opt[1]), 'if ( x >=', str(x_proc[0]), ') else 1/0'])\n",
    "    matplotlib_fits.append(load_fit)\n",
    "    matplotlib_fits.append(comm_fit)\n",
    "    matplotlib_fits.append(comp_fit)\n",
    "    return gnuplot_fits, matplotlib_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(file_path=FILE_OUT)\n",
    "def create_gnuplot(limit, para_fit, gnuplot_fits, glob_fit, points_data, cfgs_path, file_path):\n",
    "    \"\"\"Create Gnuplot file.\"\"\"\n",
    "    gp_template = os.path.join(cfgs_path, 'modelfactors.gp')\n",
    "    content = []\n",
    "    with open(gp_template) as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "    # Unroll fits\n",
    "    load_fit, comm_fit, comp_fit = gnuplot_fits\n",
    "     \n",
    "    # Replace xrange\n",
    "    content = [line.replace('#REPLACE_BY_XRANGE', ''.join(['set xrange [1:',limit,']']) ) for line in content]\n",
    "\n",
    "    # Replace projection functions\n",
    "    content = [line.replace('#REPLACE_BY_PARA_FUNCTION', para_fit ) for line in content]\n",
    "    content = [line.replace('#REPLACE_BY_LOAD_FUNCTION', load_fit ) for line in content]\n",
    "    content = [line.replace('#REPLACE_BY_COMM_FUNCTION', comm_fit ) for line in content]\n",
    "    content = [line.replace('#REPLACE_BY_COMP_FUNCTION', comp_fit ) for line in content]\n",
    "    content = [line.replace('#REPLACE_BY_GLOB_FUNCTION', glob_fit ) for line in content]\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.writelines(content)\n",
    "        \n",
    "    x_proc, y_para, y_load, y_comm, y_comp, y_glob, number_traces = points_data\n",
    "        \n",
    "    # Add data points to gnuplot file\n",
    "    with open(file_path, 'a') as f:\n",
    "        for index in range(0, number_traces):\n",
    "            line = ' '.join([str(x_proc[index]), str(y_para[index]), '\\n'])\n",
    "            f.write(line)\n",
    "        f.write('e\\n')\n",
    "\n",
    "        for index in range(0, number_traces):\n",
    "            line = ' '.join([str(x_proc[index]), str(y_load[index]), '\\n'])\n",
    "            f.write(line)\n",
    "        f.write('e\\n')\n",
    "\n",
    "        for index in range(0, number_traces):\n",
    "            line = ' '.join([str(x_proc[index]), str(y_comm[index]), '\\n'])\n",
    "            f.write(line)\n",
    "        f.write('e\\n')\n",
    "\n",
    "        for index in range(0, number_traces):\n",
    "            line = ' '.join([str(x_proc[index]), str(y_comp[index]), '\\n'])\n",
    "            f.write(line)\n",
    "        f.write('e\\n')\n",
    "\n",
    "        for index in range(0, number_traces):\n",
    "            line = ' '.join([str(x_proc[index]), str(y_glob[index]), '\\n'])\n",
    "            f.write(line)\n",
    "        f.write('e\\n')\n",
    "\n",
    "        f.write('\\n')\n",
    "        f.write('pause -1\\n')\n",
    "\n",
    "    print('Projection written to ' + file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = None\n",
    "comm = None\n",
    "comp = None\n",
    "para = None\n",
    "glob = None\n",
    "\n",
    "@task(file_path=FILE_OUT, returns=1)\n",
    "def create_matplotlib(limit, para_fit, matplotlib_fits, glob_fit, points_data, cfgs_path, file_path):\n",
    "    \"\"\"\n",
    "    Creates matplotlib figure.\n",
    "    If not a task, it can directly show the figure.\n",
    "    \"\"\"\n",
    "    # Limit cast\n",
    "    limit = int(limit)\n",
    "    # Unroll fits\n",
    "    load_fit, comm_fit, comp_fit = matplotlib_fits\n",
    "    # Unroll data\n",
    "    x_proc, y_para, y_load, y_comm, y_comp, y_glob, number_traces = points_data\n",
    "    # Define global for lambdas - needed since some of them rely on others and need to be global\n",
    "    global load\n",
    "    global comm\n",
    "    global comp\n",
    "    global para\n",
    "    global glob\n",
    "    \n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    matplotlib.rcParams[\"font.size\"] = 10\n",
    "\n",
    "    # Set x axis ticks\n",
    "    x_axis = x_proc\n",
    "\n",
    "    # Plot data\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    ax.plot(x_axis, y_para, 'ro', label='Parallel Efficiency', markersize=6)\n",
    "    ax.plot(x_axis, y_load, 'gv', label='Load Balance', markersize=6)\n",
    "    ax.plot(x_axis, y_comm, 'b^', label='Communication Efficiency', markersize=6)\n",
    "    ax.plot(x_axis, y_comp, 'ms', label='Computation Scalability', markersize=6)\n",
    "    ax.plot(x_axis, y_glob, 'y*', label='Global Efficiency', markersize=8)\n",
    "    \n",
    "    # Get fit functions\n",
    "    load = lambda x: eval(load_fit)\n",
    "    comm = lambda x: eval(comm_fit)\n",
    "    comp = lambda x: eval(comp_fit)\n",
    "    para = lambda x: eval(para_fit)\n",
    "    glob = lambda x: eval(glob_fit)\n",
    "    \n",
    "    # Plot fit lines\n",
    "    fit_x_axis = range(int(x_proc[0]), limit, 16)\n",
    "    load_fit_values = [load(x) for x in fit_x_axis]\n",
    "    comm_fit_values = [comm(x) for x in fit_x_axis]\n",
    "    comp_fit_values = [comp(x) for x in fit_x_axis]\n",
    "    para_fit_values = [para(x) for x in fit_x_axis]\n",
    "    glob_fit_values = [glob(x) for x in fit_x_axis]\n",
    "    ax.plot(fit_x_axis, para_fit_values, 'r--', label='Fit Parallel Efficiency')\n",
    "    ax.plot(fit_x_axis, load_fit_values, 'g--', label='Fit Load Balance')\n",
    "    ax.plot(fit_x_axis, comm_fit_values, 'b--', label='Fit Communication Efficiency')\n",
    "    ax.plot(fit_x_axis, comp_fit_values, 'm--', label='Fit Computation Scalability')\n",
    "    ax.plot(fit_x_axis, glob_fit_values, 'y--', label='Fit Global Efficiency')\n",
    "\n",
    "    # Plot parameters\n",
    "    ax.set(xlabel='Number of Processes', ylabel='Efficiency', title='Model factors result')\n",
    "    ax.grid()\n",
    "    ax.legend(loc=0)  # The best location (0) can be slow, so if you know the position, set it directly with 1-10 (anticlockwise starting from upper-right).\n",
    "\n",
    "    # Axis setup\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([1, limit])\n",
    "    max_y = max(max(y_para), max(y_load), max(y_comm), max(y_comp), max(y_glob))\n",
    "    axes.set_ylim([0, max_y])\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "    fig.savefig(file_path)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make two versions for the merging of data:\n",
    "#### * First: accumulation over the same object.\n",
    "#### * Second: the existing one which returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First version - Accumulating over the same object (INOUT)\n",
    "@task(dict2=INOUT, priority=True)\n",
    "def merge_data_inout(dict1, dict2):\n",
    "    \"\"\"Merge two dictionaries accumulating into dict2 - INOUT\"\"\"\n",
    "    for key in dict1.keys():\n",
    "        dict2[key].update(dict1[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second version - Returning the result of the accumulation\n",
    "@task(returns=dict, priority=True)\n",
    "def merge_data_ret(dict1, dict2):\n",
    "    \"\"\"Merge two dictionaries returning a the result of accumulating into dict2.\"\"\"\n",
    "    for key in dict1.keys():\n",
    "        dict2[key].update(dict1[key])\n",
    "    return dict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "\n",
    "In the following cell, the necessary widgets for interactive executions are defined as well as the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "class wdgts(object):\n",
    "    # List of traces to process. Accepts wild cards and automatically filters for valid traces\n",
    "    w_trace_folder = widgets.Text(value=os.getcwd() + os.path.sep + 'traces/gromacs_jesus/',\n",
    "                                  description='List of traces:',\n",
    "                                  layout={'width':'60%'})\n",
    "    # Increase output verbosity to debug level\n",
    "    w_debug = widgets.Checkbox(value=False,\n",
    "                               description='Debug')\n",
    "    # Define whether the measurements are weak or strong scaling (default: auto)\n",
    "    w_scaling = widgets.ToggleButtons(options=['auto', 'weak','strong'],\n",
    "                                      description='Scaling',\n",
    "                                      button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "                                      tooltips=['Automatic measurements scaling', 'weak measurements scaling', 'Strong measurements scaling'])\n",
    "    # Run only the projection for the given modelfactors.csv (default: false)\n",
    "    w_project = widgets.Text(value='false',\n",
    "                             placeholder='modelfactors.csv',\n",
    "                             description='CSV projection file path:',\n",
    "                             style=style,\n",
    "                             layout={'width':'60%'})\n",
    "    # Limit number of cores for the projection (default: 10000)\n",
    "    w_limit = widgets.IntText(value=10000,\n",
    "                              description='Projection # cores:',\n",
    "                              style=style,\n",
    "                              layout={'width':'60%'})\n",
    "    # Select model for prediction (default: amdahl)\n",
    "    w_model = widgets.ToggleButtons(options=['amdahl','pipe','linear'],\n",
    "                                    description='Model',\n",
    "                                    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "                                    tooltips=['Amdahl model prediction', 'Pipe model prediction', 'Linear model prediction'])\n",
    "    # Set bounds for the prediction (default: yes)\n",
    "    w_bounds = widgets.Checkbox(value=True,\n",
    "                                description='Prediction bounds')\n",
    "    # Set error restrains for prediction (default: first). first: prioritize smallest run; equal: no priority; decrease: decreasing priority for larger runs\n",
    "    w_sigma = widgets.ToggleButtons(options=['first','equal','decrease'],\n",
    "                                    description='Sigma',\n",
    "                                    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "                                    tooltips=['Prioritize smallest run', 'No priority', 'Decreasing priority for larger runs'])\n",
    "    # Path of the configuration files\n",
    "    w_cfgs = widgets.Text(value=os.getcwd() + os.path.sep + 'cfgs',\n",
    "                          placeholder='cfgs',\n",
    "                          description='Configuration files path:',\n",
    "                          style=style,\n",
    "                         layout={'width':'60%'})\n",
    "    # Path of matplotlib output file\n",
    "    w_gp_out = widgets.Text(value='results.gp',\n",
    "                            placeholder='Output_file.gp',\n",
    "                            description='Gnuplot output file:',\n",
    "                            style=style,\n",
    "                            layout={'width':'60%'})\n",
    "    # Path of matplotlib output file\n",
    "    w_mpl_out = widgets.Text(value='results.png',\n",
    "                             placeholder='Output_file.png',\n",
    "                             description='Matplotlib Output file:',\n",
    "                             style=style,\n",
    "                             layout={'width':'60%'})\n",
    "    # Path of csv output file\n",
    "    w_csv = widgets.Text(value='results.csv',\n",
    "                         placeholder='Output_file.csv',\n",
    "                         description='CSV output file:',\n",
    "                         style=style,\n",
    "                         layout={'width':'60%'})\n",
    "    # Choose reduction strategy\n",
    "    w_reduction = widgets.ToggleButtons(options=['Accumulate', 'Reduce', 'MergeReduce', 'MergeReduceAccumulate'],\n",
    "                                        description='Reductions',\n",
    "                                        button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "                                        tooltips=['Accummulate in the same loop', 'Simple reduce function', 'Reduce in pairs', 'Reduce in pairs accumulating'])\n",
    "\n",
    "\n",
    "def model_factors(trace_folder, debug, scaling, project, limit, model, bounds, sigma, cfgs, gp_out, mpl_out, csv, reduction):\n",
    "    \"\"\"\n",
    "    Main control function\n",
    "    \"\"\"\n",
    "    trace_list = []\n",
    "    for file in os.listdir(trace_folder):\n",
    "        if file.endswith(\".prv\"):\n",
    "            trace_list.append(os.path.join(trace_folder, file))\n",
    "            \n",
    "    if debug:\n",
    "        print(\"Traces :\")\n",
    "        for t in trace_list:\n",
    "            print(\"\\t- \" + str(t))\n",
    "        print(\"Debug    : \" + str(debug))\n",
    "        print(\"Scaling  : \" + str(scaling))\n",
    "        print(\"Project  : \" + str(project))\n",
    "        print(\"Limit    : \" + str(limit))\n",
    "        print(\"Model    : \" + str(model))\n",
    "        print(\"Bounds   : \" + str(bounds))\n",
    "        print(\"Sigma    : \" + str(sigma))\n",
    "        print(\"Cfgs     : \" + str(cfgs))\n",
    "        print(\"Out gp   : \" + str(gp_out))\n",
    "        print(\"Out mpl  : \" + str(mpl_out))\n",
    "        print(\"Csv      : \" + str(csv))\n",
    "        print(\"Reduction: \" + str(reduction))\n",
    "        \n",
    "    cfgs = os.path.abspath(cfgs)\n",
    "    # Check if paramedir and Dimemas are in the path\n",
    "    check_installation(debug)\n",
    "    # Check if projection-only mode is selected\n",
    "    # If not: compute everything\n",
    "    # Else: read the passed modelfactors.csv\n",
    "    if project == 'false':\n",
    "        # trace_list, trace_processes = get_traces_from_args(trace_list)\n",
    "        traces = get_traces_from_args(trace_list)\n",
    "        \n",
    "        timings       = os.path.join(cfgs, 'timings.cfg')\n",
    "        runtime       = os.path.join(cfgs, 'runtime.cfg')\n",
    "        cycles        = os.path.join(cfgs, 'cycles.cfg')\n",
    "        inst  = os.path.join(cfgs, 'instructions.cfg')\n",
    "        dimemas_cfgs = os.path.join(cfgs, 'dimemas_ideal.cfg')\n",
    "        dimemas_collectives = os.path.join(cfgs, 'dimemas.collectives')\n",
    "\n",
    "        # Compute the raw data\n",
    "        if reduction == 'Accumulate':\n",
    "            raw_data = create_empty_raw_data()\n",
    "            for name, trace in traces.items():\n",
    "                partial_raw_data = gather_raw_data(trace.get_path(), timings, runtime, cycles, inst, dimemas_cfgs, dimemas_collectives, trace.get_processes(), cfgs, debug)\n",
    "                merge_data_inout(partial_raw_data, raw_data)\n",
    "        else:\n",
    "            lraw_data = []\n",
    "            for name, trace in traces.items():\n",
    "                partial_raw_data = gather_raw_data(trace.get_path(), timings, runtime, cycles, inst, dimemas_cfgs, dimemas_collectives, trace.get_processes(), cfgs, debug)\n",
    "                lraw_data.append(partial_raw_data)\n",
    "            if reduction == 'Reduce':\n",
    "                raw_data = reduce(merge_data_ret, lraw_data)\n",
    "            elif reduction == 'MergeReduce':\n",
    "                raw_data = merge_reduce(merge_data_ret, lraw_data)\n",
    "            else:\n",
    "                # reduction == 'MergeReduceAccumulate':\n",
    "                raw_data = merge_reduce_accum(merge_data_inout, lraw_data)                    \n",
    "            \n",
    "        # Guess the weak or strong scaling\n",
    "        scaling = get_scaling_type(raw_data, traces, scaling, debug)\n",
    "\n",
    "        # Compute the model factors\n",
    "        if reduction == 'Accumulate':\n",
    "            mod_factors = create_empty_mod_factors()\n",
    "            first_trace = None\n",
    "            first_processes = None\n",
    "            for name, trace in traces.items():\n",
    "                if first_processes is None and first_trace is None:\n",
    "                    first_trace = name\n",
    "                    first_processes = trace.get_processes()\n",
    "                partial_mod_factors = compute_model_factors(raw_data, trace.get_path(), trace.get_processes(), first_trace, first_processes, scaling, debug)\n",
    "                merge_data_inout(partial_mod_factors, mod_factors)\n",
    "        else:\n",
    "            lmod_factors = []\n",
    "            first_trace = None\n",
    "            first_processes = None\n",
    "            for name, trace in traces.items():\n",
    "                if first_processes is None and first_trace is None:\n",
    "                    first_trace = name\n",
    "                    first_processes = trace.get_processes()\n",
    "                partial_mod_factors = compute_model_factors(raw_data, trace.get_path(), trace.get_processes(), first_trace, first_processes, scaling, debug)\n",
    "                lmod_factors.append(partial_mod_factors)\n",
    "            if reduction == 'Reduce':\n",
    "                mod_factors = reduce(merge_data_ret, lmod_factors)\n",
    "            elif reduction == 'MergeReduce':\n",
    "                mod_factors = merge_reduce(merge_data_ret, lmod_factors)\n",
    "            else:\n",
    "                # reduction == 'MergeReduceAccumulate':\n",
    "                mod_factors = merge_reduce_accum(merge_data_inout, lmod_factors)\n",
    "        \n",
    "        # Print the model factor results\n",
    "        mod_factors = compss_wait_on(mod_factors)\n",
    "        raw_data = compss_wait_on(raw_data)\n",
    "        print_raw_data_table_html(raw_data, traces)        # Remove _html for normal print\n",
    "        print_mod_factors_table_html(mod_factors, traces)  # Remove _html for normal print\n",
    "        print_mod_factors_csv(mod_factors, raw_data, traces, csv)\n",
    "    else:\n",
    "        # Read the model factors from the csv file\n",
    "        mod_factors, trace_list, trace_processes = read_mod_factors_csv(debug, project)\n",
    "\n",
    "    print(\"Compute_projection\")\n",
    "    fig = compute_projection(mod_factors, traces, debug, model, limit, bounds, sigma, gp_out, mpl_out, cfgs)\n",
    "    fig = compss_wait_on(fig)\n",
    "    fig.show()\n",
    "    \n",
    "widgets.interact_manual(model_factors, trace_folder=wdgts.w_trace_folder, debug=wdgts.w_debug, scaling=wdgts.w_scaling, project=wdgts.w_project, limit=wdgts.w_limit, model=wdgts.w_model, bounds=wdgts.w_bounds, sigma=wdgts.w_sigma, cfgs=wdgts.w_cfgs, gp_out=wdgts.w_gp_out, mpl_out=wdgts.w_mpl_out, csv=wdgts.w_csv, reduction=wdgts.w_reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipycompss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
