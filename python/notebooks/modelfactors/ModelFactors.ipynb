{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Factors\n",
    "\n",
    "## Generates performance metrics from a set of Paraver traces\n",
    "\n",
    "\n",
    "__author__ = \"Michael Wagner\"\n",
    "\n",
    "__copyright__ = \"Copyright 2017, Barcelona Supercomputing Center (BSC)\"\n",
    "\n",
    "__version__ = 0.3.6 +"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global imports\n",
    "import os\n",
    "import fnmatch\n",
    "import time\n",
    "import numpy\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Externally defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import Trace\n",
    "from helpers import check_installation\n",
    "from helpers import human_readable\n",
    "from helpers import run_command\n",
    "from helpers import save_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyCOMPSs imports\n",
    "import pycompss.interactive as ipycompss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the COMPSs runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipycompss.start(graph=True, debug=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycompss.api.task import task\n",
    "from pycompss.api.constraint import constraint\n",
    "from pycompss.api.parameter import *\n",
    "from pycompss.api.api import compss_wait_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contains all raw data entries with a printable name.\n",
    "#This is used to generate and print all raw data, so, if an entry is added, it should be added here, too.\n",
    "raw_data_doc = OrderedDict([('runtime', 'Runtime (us)'), ('runtime_dim', 'Runtime (ideal)'), ('useful_avg', 'Useful duration (average)'), ('useful_max', 'Useful duration (maximum)'), ('useful_tot', 'Useful duration (total)'), ('useful_dim', 'Useful duration (ideal, max)'), ('useful_ins', 'Useful instructions (total)'), ('useful_cyc', 'Useful cycles (total)')])\n",
    "\n",
    "#Contains all model factor entries with a printable name.\n",
    "#This is used to generate and print all model factors, so, if an entry is added, it should be added here, too.\n",
    "mod_factors_doc = OrderedDict([('parallel_eff', 'Parallel efficiency'), ('load_balance', '  Load balance'), ('comm_eff', '  Communication efficiency'), ('serial_eff', '    Serialization efficiency'), ('transfer_eff', '    Transfer efficiency'), ('comp_scale', 'Computation scalability'), ('global_eff', 'Global efficiency'), ('ipc_scale', 'IPC scalability'), ('inst_scale', 'Instruction scalability'), ('freq_scale', 'Frequency scalability'), ('speedup', 'Speedup'), ('ipc', 'Average IPC'), ('freq', 'Average frequency (GHz)')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mod_factors_csv(debug, project):\n",
    "    \"\"\"Reads the model factors table from a csv file.\"\"\"\n",
    "    global mod_factors_doc\n",
    "    delimiter = ';'\n",
    "    file_path = project\n",
    "\n",
    "    #Read csv to list of lines\n",
    "    if os.path.isfile(file_path) and file_path[-4:] == '.csv':\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [line.rstrip('\\n') for line in lines]\n",
    "    else:\n",
    "        raise Exception('==ERROR==', file_path, 'is not a valid csv file.')\n",
    "\n",
    "    #Get the number of processes of the traces\n",
    "    processes = lines[0].split(delimiter)\n",
    "    processes.pop(0)\n",
    "\n",
    "    #Create artificial trace_list and trace_processes\n",
    "    trace_list = []\n",
    "    trace_processes = {}\n",
    "    for process in processes:\n",
    "        trace_list.append(process)\n",
    "        trace_processes[process] = int(process)\n",
    "\n",
    "    #Create empty mod_factors handle\n",
    "    mod_factors = create_mod_factors(trace_list)\n",
    "\n",
    "    #Get mod_factor_doc keys\n",
    "    mod_factors_keys = list(mod_factors_doc.items())\n",
    "\n",
    "    #Iterate over the data lines\n",
    "    for index, line in enumerate(lines[1:len(mod_factors_keys)+1]):\n",
    "        key = mod_factors_keys[index][0]\n",
    "        line = line.split(delimiter)\n",
    "        for index, trace in enumerate(trace_list):\n",
    "            mod_factors[key][trace] = float(line[index+1])\n",
    "\n",
    "    if debug:\n",
    "        print_mod_factors_table(mod_factors, trace_list, trace_processes)\n",
    "\n",
    "    return mod_factors, trace_list, trace_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amdahl(x, x0, f):\n",
    "    \"\"\"#Projection function based on amdahl; 2 degrees of freedom: x0, f\"\"\"\n",
    "    return x0 / (f + (1 - f) * x)\n",
    "\n",
    "def pipe(x, x0, f):\n",
    "    \"\"\"Projection function based on pipeline; 2 degrees of freedom: x0, f\"\"\"\n",
    "    return x0 * x / ((1 - f) + f * (2 * x - 1) )\n",
    "\n",
    "def linear(x, x0, f):\n",
    "    \"\"\"Projection function linear; 2 degrees of freedom: x0, a\"\"\"\n",
    "    return x0 + f * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_projection(mod_factors, traces, debug, model, limit, bounds, sigma, out, cfgs):\n",
    "    \"\"\"Computes the projection from the gathered model factors and returns the\n",
    "    according dictionary of fitted prediction functions.\"\"\"\n",
    "    \n",
    "    trace_list, trace_processes = get_list_proc(traces)\n",
    "\n",
    "    if debug:\n",
    "        print('==DEBUG== Computing projection of model factors.')\n",
    "\n",
    "    number_traces = len(trace_list)\n",
    "    x_proc = numpy.zeros(number_traces)\n",
    "    y_para = numpy.zeros(number_traces)\n",
    "    y_load = numpy.zeros(number_traces)\n",
    "    y_comm = numpy.zeros(number_traces)\n",
    "    y_comp = numpy.zeros(number_traces)\n",
    "    y_glob = numpy.zeros(number_traces)\n",
    "\n",
    "    #Convert dictionaries to NumPy arrays\n",
    "    for index, trace in enumerate(trace_list):\n",
    "        x_proc[index] = trace_processes[trace]\n",
    "        y_para[index] = mod_factors['parallel_eff'][trace]\n",
    "        y_load[index] = mod_factors['load_balance'][trace]\n",
    "        y_comm[index] = mod_factors['comm_eff'][trace]\n",
    "        y_comp[index] = mod_factors['comp_scale'][trace]\n",
    "        y_glob[index] = mod_factors['global_eff'][trace]\n",
    "\n",
    "    #Select model function\n",
    "    if model == 'amdahl':\n",
    "        _model = amdahl\n",
    "    elif model == 'pipe':\n",
    "        _model = pipe\n",
    "    elif model == 'linear':\n",
    "        _model = linear\n",
    "\n",
    "    #Set limit for projection\n",
    "    if limit:\n",
    "        _limit = str(limit)\n",
    "    else:\n",
    "        _limit = '10000'\n",
    "\n",
    "    #Set boundary for curve fitting parameters: ([x0_min,f_min],[x0_max,f_max])\n",
    "    #For amdahl and pipe f is in [0,1]\n",
    "    if bounds:\n",
    "        _bounds = ([-numpy.inf,0],[numpy.inf,1])\n",
    "    else:\n",
    "        _bounds = ([-numpy.inf,-numpy.inf],[numpy.inf,numpy.inf])\n",
    "\n",
    "    #Set data uncertainty for vector with y-values.\n",
    "    #Smaller values mean higher priority for these y-values.\n",
    "    #Values are compared relatively, not absolute.\n",
    "    if sigma == 'first':\n",
    "        _sigma = numpy.ones(number_traces)\n",
    "        _sigma[0] = 0.1\n",
    "    elif sigma == 'equal':\n",
    "        _sigma = numpy.ones(number_traces)\n",
    "    elif sigma == 'decrease':\n",
    "        _sigma = numpy.linspace(1, 2, number_traces)\n",
    "\n",
    "    # Execute curve fitting, returns optimal parameters array and covariance matrix\n",
    "    # Uses a Levenberg-Marquardt algorithm, i.e. damped least-squares, if no\n",
    "    # bounds are provide; otherwise a Trust Region Reflective algorithm.\n",
    "    # Please note: Both are not true least squares.\n",
    "    # They are greedy methoda and simply run into the nearest local minimum.\n",
    "    # However, this should work fine for this simple 1D optimization.\n",
    "    # Use try to check for SciPy version.\n",
    "    try:\n",
    "        para_opt, para_cov = scipy.optimize.curve_fit(_model, x_proc, y_para, sigma=_sigma, bounds=_bounds)\n",
    "        load_opt, load_cov = scipy.optimize.curve_fit(_model, x_proc, y_load, sigma=_sigma, bounds=_bounds)\n",
    "        comm_opt, comm_cov = scipy.optimize.curve_fit(_model, x_proc, y_comm, sigma=_sigma, bounds=_bounds)\n",
    "        comp_opt, comp_cov = scipy.optimize.curve_fit(_model, x_proc, y_comp, sigma=_sigma, bounds=_bounds)\n",
    "        glob_opt, glob_cov = scipy.optimize.curve_fit(_model, x_proc, y_glob, sigma=_sigma, bounds=_bounds)\n",
    "    except TypeError:\n",
    "        print('==Error== Projection failed! The script requires SciPy 0.17.0 or newer.')\n",
    "        return\n",
    "\n",
    "    #Create the fitting functions for gnuplot; 2 degrees of freedom: x0, f\n",
    "    if _model == amdahl:\n",
    "        load_fit, comm_fit, comp_fit = fit_amdahl(x_proc, load_opt, comm_opt, comp_opt)\n",
    "    elif _model == pipe:\n",
    "        load_fit, comm_fit, comp_fit = fit_pipe(x_proc, load_opt, comm_opt, comp_opt)\n",
    "    elif _model == linear:\n",
    "        load_fit, comm_fit, comp_fit = fit_linear(x_proc, load_opt, comm_opt, comp_out)\n",
    "\n",
    "    #Select whether para and glob are fitted or multiplied according to model\n",
    "    para_fit = ' '.join(['para( x ) = load( x ) * comm( x ) / 100'])   \n",
    "    glob_fit = ' '.join(['glob( x ) = para( x ) * comp( x ) / 100'])\n",
    "\n",
    "    if not os.path.exists(os.path.dirname(out)):\n",
    "        os.makedirs(os.path.dirname(out))\n",
    "\n",
    "    outfile_path = out # os.path.join(out, 'modelfactors.gp')\n",
    "    cfgs_path = cfgs\n",
    "    points_data = [x_proc, y_para, y_load, y_comm, y_comp, y_glob, number_traces]\n",
    "    create_gnuplot(_limit, para_fit, load_fit, comm_fit, comp_fit, glob_fit, points_data, cfgs_path, outfile_path)\n",
    "    # write_projection(x_proc, y_para, y_load, y_comm, y_comp, y_glob, number_traces, outfile_path)  # collapsed with previous\n",
    "    plot = create_matplotlib(_limit, para_fit, load_fit, comm_fit, comp_fit, glob_fit, points_data, cfgs_path, outfile_path)\n",
    "    show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show(plot):\n",
    "    plot.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_traces_from_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_data(trace_name):\n",
    "    \"\"\"Creates 2D dictionary of the raw input data and initializes with zero.\n",
    "    The raw_data dictionary has the format: [raw data key][trace].\n",
    "    \"\"\"\n",
    "    global raw_data_doc\n",
    "    raw_data = {}\n",
    "    for key in raw_data_doc:\n",
    "        trace_dict = {}\n",
    "        trace_dict[trace_name] = 0\n",
    "        raw_data[key] = trace_dict\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mod_factors(trace_name):\n",
    "    \"\"\"Creates 2D dictionary of the model factors and initializes with an empty\n",
    "    string. The mod_factors dictionary has the format: [mod factor key][trace].\n",
    "    \"\"\"\n",
    "    global mod_factors_doc\n",
    "    mod_factors = {}\n",
    "    for key in mod_factors_doc:\n",
    "        trace_dict = {}\n",
    "        trace_dict[trace_name] = 0.0\n",
    "        mod_factors[key] = trace_dict\n",
    "    return mod_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_proc(traces):\n",
    "    trace_list = traces.keys()\n",
    "    trace_processes = {}\n",
    "    for trace_name in trace_list:\n",
    "        trace_processes[trace_name] = traces[trace_name].get_processes()\n",
    "    return trace_list, trace_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_raw_data_table(raw_data, traces):\n",
    "    \"\"\"Prints the raw data table in human readable form on stdout.\"\"\"\n",
    "    global raw_data_doc\n",
    "    print('Overview of the collected raw data:')\n",
    "    \n",
    "    trace_list, trace_processes = get_list_proc(traces)\n",
    "    \n",
    "    longest_name = len(sorted(raw_data_doc.values(), key=len)[-1])\n",
    "\n",
    "    line = ''.rjust(longest_name)\n",
    "    for trace in trace_list:\n",
    "        line += ' | '\n",
    "        line += str(trace_processes[trace]).rjust(15)\n",
    "    print(line)\n",
    "\n",
    "    print(''.ljust(len(line),'='))\n",
    "    \n",
    "    for data_key in raw_data_doc:\n",
    "        line = raw_data_doc[data_key].ljust(longest_name)\n",
    "        for trace in trace_list:\n",
    "            line += ' | '\n",
    "            line += str(raw_data[data_key][trace]).rjust(15)\n",
    "        print(line)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_raw_data_table_html(raw_data, traces):\n",
    "    \"\"\"Prints the raw data table in html form.\"\"\"\n",
    "    global raw_data_doc\n",
    "    print('Overview of the collected raw data:')\n",
    "    \n",
    "    trace_list, trace_processes = get_list_proc(traces)\n",
    "    \n",
    "    headers = ['Parameter']\n",
    "    data = []\n",
    "\n",
    "    for trace in trace_list:\n",
    "        headers.append(str(trace_processes[trace]))\n",
    "    \n",
    "    for data_key in raw_data_doc:\n",
    "        line = []\n",
    "        line.append(raw_data_doc[data_key])\n",
    "        for trace in trace_list:\n",
    "            line.append(str(raw_data[data_key][trace]))\n",
    "        data.append(line)\n",
    "    \n",
    "    from IPython.display import HTML, display\n",
    "    import tabulate\n",
    "    display(HTML(tabulate.tabulate(data, headers=headers, tablefmt='html', floatfmt=\".2f\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mod_factors_table(mod_factors, traces):\n",
    "    \"\"\"Prints the model factors table in human readable form on stdout.\"\"\"\n",
    "    global mod_factors_doc\n",
    "    print('Overview of the computed model factors:')\n",
    "\n",
    "    longest_name = len(sorted(mod_factors_doc.values(), key=len)[-1])\n",
    "    \n",
    "    trace_list, trace_processes = get_list_proc(traces)\n",
    "    \n",
    "    line = ''.rjust(longest_name)\n",
    "    for trace in trace_list:\n",
    "        line += ' | '\n",
    "        line += str(trace_processes[trace]).rjust(10)\n",
    "    print(line)\n",
    "\n",
    "    print(''.ljust(len(line),'='))\n",
    "\n",
    "    for mod_key in mod_factors_doc:\n",
    "        line = mod_factors_doc[mod_key].ljust(longest_name)\n",
    "        if mod_key in ['speedup','ipc','freq']:\n",
    "            for trace in trace_list:\n",
    "                line += ' | '\n",
    "                try: #except NaN\n",
    "                    line += ('{0:.2f}'.format(mod_factors[mod_key][trace])).rjust(10)\n",
    "                except ValueError:\n",
    "                    line += ('{}'.format(mod_factors[mod_key][trace])).rjust(10)\n",
    "        else:\n",
    "            for trace in trace_list:\n",
    "                line += ' | '\n",
    "                try: # except NaN\n",
    "                    line += ('{0:.2f}%'.format(mod_factors[mod_key][trace])).rjust(10)\n",
    "                except ValueError:\n",
    "                    line += ('{}'.format(mod_factors[mod_key][trace])).rjust(10)\n",
    "        print(line)\n",
    "        # Print empty line to separate values\n",
    "        if mod_key in ['global_eff','freq_scale']:\n",
    "            line = ''.ljust(longest_name)\n",
    "            for trace in trace_list:\n",
    "                line += ' | '\n",
    "                line += ''.rjust(10)\n",
    "            print(line)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mod_factors_table_html(mod_factors, traces):\n",
    "    \"\"\"Prints the model factors table in html form.\"\"\"\n",
    "    global mod_factors_doc\n",
    "    print('Overview of the computed model factors:')\n",
    "\n",
    "    trace_list, trace_processes = get_list_proc(traces)\n",
    "    \n",
    "    headers = ['Parameter']\n",
    "    data = []\n",
    "    \n",
    "    for trace in trace_list:\n",
    "        headers.append(str(trace_processes[trace]))\n",
    "\n",
    "    for mod_key in mod_factors_doc:\n",
    "        line = []\n",
    "        line.append(mod_factors_doc[mod_key])\n",
    "        for trace in trace_list:\n",
    "            line.append(mod_factors[mod_key][trace])\n",
    "        data.append(line)\n",
    "\n",
    "    from IPython.display import HTML, display\n",
    "    import tabulate\n",
    "    display(HTML(tabulate.tabulate(data, headers=headers, tablefmt='html', floatfmt=\".2f\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mod_factors_csv(mod_factors, raw_data, traces, file_path):\n",
    "    \"\"\"Prints the model factors table in a csv file.\"\"\"\n",
    "    global mod_factors_doc\n",
    "    \n",
    "    trace_list, trace_processes = get_list_proc(traces)\n",
    "\n",
    "    delimiter = ';'\n",
    "    with open(file_path, 'w') as output:\n",
    "        line = 'Number of processes'\n",
    "        for trace in trace_list:\n",
    "            line += delimiter\n",
    "            line += str(trace_processes[trace])\n",
    "        output.write(line + '\\n')\n",
    "\n",
    "        for mod_key in mod_factors_doc:\n",
    "            line = mod_factors_doc[mod_key].replace('  ', '', 2)\n",
    "            # for trace in trace_list:\n",
    "            for trace in mod_factors[mod_key]:\n",
    "                line += delimiter\n",
    "                try: # except NaN\n",
    "                    line += '{0:.6f}'.format(mod_factors[mod_key][trace])\n",
    "                except ValueError:\n",
    "                    line += '{}'.format(mod_factors[mod_key][trace])\n",
    "            output.write(line + '\\n')\n",
    "\n",
    "        output.write('#\\n')\n",
    "\n",
    "        for raw_key in raw_data_doc:\n",
    "            line = '#' + raw_data_doc[raw_key]\n",
    "            # for trace in trace_list:\n",
    "            for trace in raw_data[raw_key]:\n",
    "                line += delimiter\n",
    "                try: # except NaN\n",
    "                    line += '{0:.2f}'.format(raw_data[raw_key][trace])\n",
    "                except ValueError:\n",
    "                    line += '{}'.format(raw_data[raw_key][trace])\n",
    "            output.write(line + '\\n')\n",
    "\n",
    "    print('Model factors written to ' + file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ideal_trace(trace, processes, cfg_dir, debug):\n",
    "    \"\"\"Runs prv2dim and dimemas with ideal configuration for given trace.\"\"\"\n",
    "    trace_dim = trace[:-4] + '.dim'\n",
    "    trace_sim = trace[:-4] + '.sim.prv'\n",
    "    cmd = ['prv2dim', trace, trace_dim]\n",
    "    run_command(cmd, debug)\n",
    "\n",
    "    if os.path.isfile(trace_dim):\n",
    "        if debug:\n",
    "            print('==DEBUG== Created file ' + trace_dim)\n",
    "    else:\n",
    "        print('==Error== ' + trace_dim + 'could not be creaeted.')\n",
    "        return\n",
    "\n",
    "    #Create Dimemas configuration\n",
    "    content = []\n",
    "    with open(os.path.join(cfg_dir, 'dimemas_ideal.cfg')) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    content = [line.replace('REPLACE_BY_NTASKS', str(processes) ) for line in content]\n",
    "    content = [line.replace('REPLACE_BY_COLLECTIVES_PATH', os.path.join(cfg_dir, 'dimemas.collectives')) for line in content]\n",
    "\n",
    "    with open(trace[:-4]+'.dimemas_ideal.cfg', 'w') as f:\n",
    "        f.writelines(content)\n",
    "\n",
    "    cmd = ['Dimemas', '-S', '32k', '--dim', trace_dim, '-p', trace_sim, trace[:-4]+'.dimemas_ideal.cfg']\n",
    "    run_command(cmd, debug)\n",
    "\n",
    "    os.remove(trace_dim)\n",
    "    os.remove(trace[:-4]+'.dimemas_ideal.cfg')\n",
    "\n",
    "    if os.path.isfile(trace_sim):\n",
    "        if debug:\n",
    "            print('==DEBUG== Created file ' + trace_sim)\n",
    "        return trace_sim\n",
    "    else:\n",
    "        print('==Error== ' + trace_sim + ' could not be creaeted.')\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(returns=str)\n",
    "def get_scaling_type(raw_data, traces, scaling, debug):\n",
    "    #def get_scaling_type(raw_data, trace_list, trace_processes, first_trace_processes, scaling, debug):\n",
    "    \"\"\"Guess the scaling type (weak/strong) based on the useful instructions.\n",
    "    Computes the normalized instruction ratio for all measurements, whereas the\n",
    "    normalized instruction ratio is (instructions ratio / process ratio) with\n",
    "    the smallest run as reference. For exact weak scaling the normalized ratio\n",
    "    should be exactly 1 and for exact strong scaling it should be close to zero\n",
    "    with an upper bound of 0.5. The eps value defines the threshold to be\n",
    "    considered weak scaling and should give enough buffer to safely handle\n",
    "    non-ideal scaling.\n",
    "    \"\"\"\n",
    "    eps = 0.9\n",
    "    normalized_inst_ratio = 0\n",
    "    \n",
    "    trace_list = traces.keys()\n",
    "    first_trace_processes = None\n",
    "    trace_processes = []\n",
    "    for trace_name in trace_list:\n",
    "        trace_processes = traces[trace_name].get_processes()\n",
    "        if first_trace_processes is None:\n",
    "            first_trace_processes = trace_processes\n",
    "\n",
    "    #Check if there is only one trace.\n",
    "    if len(trace_list) == 1:\n",
    "        return 'strong'\n",
    "\n",
    "    for trace in trace_list:\n",
    "        inst_ratio = float(raw_data['useful_ins'][trace]) / float(raw_data['useful_ins'][trace_list[0]])\n",
    "        # proc_ratio = float(trace_processes[trace]) / float(trace_processes[trace_list[0]])\n",
    "        proc_ratio = float(trace_processes) / float(first_trace_processes)\n",
    "        normalized_inst_ratio += inst_ratio / proc_ratio\n",
    "\n",
    "    #Get the average inst increase. Ignore ratio of first trace 1.0)\n",
    "    normalized_inst_ratio = (normalized_inst_ratio - 1) / (len(trace_list) - 1)\n",
    "\n",
    "    scaling_computed = ''\n",
    "\n",
    "    if normalized_inst_ratio > eps:\n",
    "        scaling_computed = 'weak'\n",
    "    else:\n",
    "        scaling_computed = 'strong'\n",
    "\n",
    "    if scaling == 'auto':\n",
    "        if debug:\n",
    "            print('==DEBUG== Detected ' + scaling_computed + ' scaling.')\n",
    "            print('')\n",
    "        return scaling_computed\n",
    "\n",
    "    if scaling == 'weak':\n",
    "        if scaling_computed == 'strong':\n",
    "            print('==Warning== Scaling set to weak scaling but detected strong scaling.')\n",
    "            print('')\n",
    "        return 'weak'\n",
    "\n",
    "    if scaling == 'strong':\n",
    "        if scaling_computed == 'weak':\n",
    "            print('==Warning== Scaling set to strong scaling but detected weak scaling.')\n",
    "            print('')\n",
    "        return 'strong'\n",
    "\n",
    "    raise Exception('==Error== reached undefined control flow state.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(trace=FILE_IN, timings=FILE_IN, runtime=FILE_IN, cycles=FILE_IN, inst=FILE_IN, dimemas_cfgs=FILE_IN, dimemas_collectives=FILE_IN, returns=dict)\n",
    "def gather_raw_data(trace, timings, runtime, cycles, inst, dimemas_cfgs, dimemas_collectives, trace_processes, cfgs_path, debug):\n",
    "    \"\"\"Gathers all raw data needed to generate the model factors. Return raw\n",
    "    data in a 2D dictionary <data type><list of values for each trace>\"\"\"\n",
    "    trace_name = os.path.basename(trace)\n",
    "    raw_data = create_raw_data(trace_name)\n",
    "\n",
    "    cfgs = {}\n",
    "    cfgs['root_dir']      = cfgs_path\n",
    "    cfgs['timings']       = os.path.join(cfgs['root_dir'], 'timings.cfg')\n",
    "    cfgs['runtime']       = os.path.join(cfgs['root_dir'], 'runtime.cfg')\n",
    "    cfgs['cycles']        = os.path.join(cfgs['root_dir'], 'cycles.cfg')\n",
    "    cfgs['instructions']  = os.path.join(cfgs['root_dir'], 'instructions.cfg')\n",
    "\n",
    "    #Main loop over all traces\n",
    "    time_tot = time.time()\n",
    "\n",
    "    line = 'Analyzing ' + os.path.basename(trace)\n",
    "    line += ' (' + str(trace_processes) + ' processes'\n",
    "    line += ', ' + human_readable( os.path.getsize( trace ) ) + ')'\n",
    "    print(line)\n",
    "\n",
    "    #Create simulated ideal trace with Dimemas\n",
    "    time_dim = time.time()\n",
    "    trace_sim = create_ideal_trace(trace, trace_processes, cfgs['root_dir'], debug)\n",
    "    time_dim = time.time() - time_dim\n",
    "    if not trace_sim == '':\n",
    "        print('Successfully created simulated trace with Dimemas in {0:.1f} seconds.'.format(time_dim))\n",
    "    else:\n",
    "        print('Failed to create simulated trace with Dimemas.')\n",
    "\n",
    "    #Run paramedir for the original and simulated trace\n",
    "    time_pmd = time.time()\n",
    "    cmd_normal = ['paramedir', trace]\n",
    "    cmd_normal.extend([cfgs['timings'],      trace[:-4] + '.timings.stats'])\n",
    "    cmd_normal.extend([cfgs['runtime'],      trace[:-4] + '.runtime.stats'])\n",
    "    cmd_normal.extend([cfgs['cycles'],       trace[:-4] + '.cycles.stats'])\n",
    "    cmd_normal.extend([cfgs['instructions'], trace[:-4] + '.instructions.stats'])\n",
    "\n",
    "    cmd_ideal = ['paramedir', trace_sim]\n",
    "    cmd_ideal.extend([cfgs['timings'],       trace_sim[:-4] + '.timings.stats'])\n",
    "    cmd_ideal.extend([cfgs['runtime'],       trace_sim[:-4] + '.runtime.stats'])\n",
    "\n",
    "    run_command(cmd_normal, debug)\n",
    "    if not trace_sim == '':\n",
    "        run_command(cmd_ideal, debug)\n",
    "\n",
    "    time_pmd = time.time() - time_pmd\n",
    "\n",
    "    error_timing = 0;\n",
    "    error_counters = 0;\n",
    "    error_ideal = 0;\n",
    "\n",
    "    #Check if all files are created\n",
    "    if not os.path.exists(trace[:-4] + '.timings.stats') or \\\n",
    "       not os.path.exists(trace[:-4] + '.runtime.stats'):\n",
    "        print('==ERROR== Failed to compute timing information with paramedir.')\n",
    "        error_timing = 1\n",
    "\n",
    "    if not os.path.exists(trace[:-4] + '.cycles.stats') or \\\n",
    "       not os.path.exists(trace[:-4] + '.instructions.stats'):\n",
    "        print('==ERROR== Failed to compute counter information with paramedir.')\n",
    "        error_counters = 1\n",
    "\n",
    "    if not os.path.exists(trace_sim[:-4] + '.timings.stats') or \\\n",
    "       not os.path.exists(trace_sim[:-4] + '.runtime.stats'):\n",
    "        print('==ERROR== Failed to compute timing information with paramedir.')\n",
    "        error_ideal = 1\n",
    "        trace_sim = ''\n",
    "\n",
    "    if error_timing or error_counters or error_ideal:\n",
    "        print('Failed to analyze trace with paramedir in {0:.1f} seconds.'.format(time_pmd))\n",
    "    else:\n",
    "        print('Successfully analyzed trace with paramedir in {0:.1f} seconds.'.format(time_pmd))\n",
    "\n",
    "\n",
    "    #Parse the paramedir output files\n",
    "    time_prs = time.time()\n",
    "\n",
    "    #Get total, average, and maximum useful duration\n",
    "    if os.path.exists(trace[:-4] + '.timings.stats'):\n",
    "        content = []\n",
    "        with open(trace[:-4] + '.timings.stats') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        for line in content:\n",
    "            if line.split():\n",
    "                if line.split()[0] == 'Total':\n",
    "                    raw_data['useful_tot'][trace_name] = float(line.split()[1])\n",
    "                if line.split()[0] == 'Average':\n",
    "                    raw_data['useful_avg'][trace_name] = float(line.split()[1])\n",
    "                if line.split()[0] == 'Maximum':\n",
    "                    raw_data['useful_max'][trace_name] = float(line.split()[1])\n",
    "    else:\n",
    "        raw_data['useful_tot'][trace_name] = 'NaN'\n",
    "        raw_data['useful_avg'][trace_name] = 'NaN'\n",
    "        raw_data['useful_max'][trace_name] = 'NaN'\n",
    "\n",
    "    #Get runtime\n",
    "    if os.path.exists(trace[:-4] + '.runtime.stats'):\n",
    "        content = []\n",
    "        with open(trace[:-4] + '.runtime.stats') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        for line in content:\n",
    "            if line.split():\n",
    "                if line.split()[0] == 'Average':\n",
    "                    raw_data['runtime'][trace_name] = float(line.split()[1])\n",
    "    else:\n",
    "        raw_data['runtime'][trace_name] = 'NaN'\n",
    "\n",
    "    #Get useful cycles\n",
    "    if os.path.exists(trace[:-4] + '.cycles.stats'):\n",
    "        content = []\n",
    "        with open(trace[:-4] + '.cycles.stats') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        for line in content:\n",
    "            if line.split():\n",
    "                if line.split()[0] == 'Total':\n",
    "                    raw_data['useful_cyc'][trace_name] = int(float(line.split()[1]))\n",
    "    else:\n",
    "        raw_data['useful_cyc'][trace_name] = 'NaN'\n",
    "\n",
    "    #Get useful instructions\n",
    "    if os.path.exists(trace[:-4] + '.instructions.stats'):\n",
    "        content = []\n",
    "        with open(trace[:-4] + '.instructions.stats') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        for line in content:\n",
    "            if line.split():\n",
    "                if line.split()[0] == 'Total':\n",
    "                    raw_data['useful_ins'][trace_name] = int(float(line.split()[1]))\n",
    "    else:\n",
    "        raw_data['useful_ins'][trace_name] ='NaN'\n",
    "\n",
    "    #Get maximum useful duration for simulated trace\n",
    "    if os.path.exists(trace_sim[:-4] + '.timings.stats'):\n",
    "        content = []\n",
    "        with open(trace_sim[:-4] + '.timings.stats') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        for line in content:\n",
    "            if line.split():\n",
    "                if line.split()[0] == 'Maximum':\n",
    "                    raw_data['useful_dim'][trace_name] = float(line.split()[1])\n",
    "    else:\n",
    "        raw_data['useful_dim'][trace_name] = 'NaN'\n",
    "\n",
    "    #Get runtime for simulated trace\n",
    "    if os.path.exists(trace_sim[:-4] + '.runtime.stats'):\n",
    "        content = []\n",
    "        with open(trace_sim[:-4] + '.runtime.stats') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        for line in content:\n",
    "            if line.split():\n",
    "                if line.split()[0] == 'Average':\n",
    "                    raw_data['runtime_dim'][trace_name] = float(line.split()[1])\n",
    "    else:\n",
    "        raw_data['runtime_dim'][trace_name] = 'NaN'\n",
    "\n",
    "    #Remove paramedir output files\n",
    "    save_remove(trace[:-4] + '.timings.stats', debug)\n",
    "    save_remove(trace[:-4] + '.runtime.stats', debug)\n",
    "    save_remove(trace[:-4] + '.cycles.stats', debug)\n",
    "    save_remove(trace[:-4] + '.instructions.stats', debug)\n",
    "    save_remove(trace_sim[:-4] + '.timings.stats', debug)\n",
    "    save_remove(trace_sim[:-4] + '.runtime.stats', debug)\n",
    "    time_prs = time.time() - time_prs\n",
    "\n",
    "    time_tot = time.time() - time_tot\n",
    "    print('Finished successfully in {0:.1f} seconds.'.format(time_tot))\n",
    "    print('')\n",
    "\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(returns=dict, priority=True)\n",
    "def merge_dicts(dict1, dict2):\n",
    "    dict_all = {}\n",
    "    for key in dict1.keys():\n",
    "        x = dict1[key]\n",
    "        y = dict2[key]\n",
    "        z = x.copy()   \n",
    "        z.update(y)\n",
    "        dict_all[key] = z\n",
    "    return dict_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(trace=FILE_IN, returns=dict)\n",
    "def compute_model_factors(raw_data, trace, trace_processes, first_trace, first_trace_processes, scaling, debug):\n",
    "    \"\"\"Computes the model factors from the gathered raw data and returns the\n",
    "    according dictionary of model factors.\"\"\"\n",
    "    trace_name = os.path.basename(trace)\n",
    "    mod_factors = create_mod_factors(trace_name)\n",
    "    \n",
    "    proc_ratio = float(trace_processes) / float(first_trace_processes)\n",
    "    \n",
    "    #Basic efficiency factors\n",
    "    try: #except NaN\n",
    "        mod_factors['load_balance'][trace_name] = raw_data['useful_avg'][trace_name] / raw_data['useful_max'][trace_name] * 100.0\n",
    "    except:\n",
    "        mod_factors['load_balance'][trace_name] = 'NaN'\n",
    "\n",
    "    try: #except NaN\n",
    "        mod_factors['comm_eff'][trace_name] = raw_data['useful_max'][trace_name] / raw_data['runtime'][trace_name] * 100.0\n",
    "    except:\n",
    "        mod_factors['comm_eff'][trace_name] = 'NaN'\n",
    "\n",
    "    try: #except NaN\n",
    "        mod_factors['serial_eff'][trace_name] = raw_data['useful_dim'][trace_name] / raw_data['runtime_dim'][trace_name] * 100.0\n",
    "    except:\n",
    "        mod_factors['serial_eff'][trace_name] = 'NaN'\n",
    "\n",
    "    try: #except NaN\n",
    "        mod_factors['transfer_eff'][trace_name] = mod_factors['comm_eff'][trace_name] / mod_factors['serial_eff'][trace_name] * 100.0\n",
    "    except:\n",
    "        mod_factors['transfer_eff'][trace_name] = 'NaN'\n",
    "\n",
    "    try: #except NaN\n",
    "        mod_factors['parallel_eff'][trace_name] = mod_factors['load_balance'][trace_name] * mod_factors['comm_eff'][trace_name] / 100.0\n",
    "    except:\n",
    "        mod_factors['parallel_eff'][trace_name] = 'NaN'\n",
    "\n",
    "    try: #except NaN\n",
    "        if scaling == 'strong':\n",
    "            mod_factors['comp_scale'][trace_name] = raw_data['useful_tot'][first_trace] / raw_data['useful_tot'][trace_name] * 100.0\n",
    "        else:\n",
    "            mod_factors['comp_scale'][trace_name] = raw_data['useful_tot'][first_trace] / raw_data['useful_tot'][trace_name] * proc_ratio * 100.0\n",
    "    except:\n",
    "        mod_factors['comp_scale'][trace_name] = 'NaN'\n",
    "\n",
    "    try: #except NaN\n",
    "        mod_factors['global_eff'][trace_name] = mod_factors['parallel_eff'][trace_name] * mod_factors['comp_scale'][trace_name] / 100.0\n",
    "    except:\n",
    "        mod_factors['global_eff'][trace_name] = 'NaN'\n",
    "\n",
    "    #Basic scalability factors\n",
    "    try: #except NaN\n",
    "        mod_factors['ipc'][trace_name] = float(raw_data['useful_ins'][trace_name]) / float(raw_data['useful_cyc'][trace_name])\n",
    "    except:\n",
    "        mod_factors['ipc'][trace_name] = 'NaN'\n",
    "    try: #except NaN\n",
    "        ipc_first_trace = float(raw_data['useful_ins'][first_trace]) / float(raw_data['useful_cyc'][first_trace])\n",
    "        mod_factors['ipc_scale'][trace_name] = mod_factors['ipc'][trace_name] / ipc_first_trace * 100.0\n",
    "        # Do not reuse mod_factors for the first_trace to avoid INOUT\n",
    "        # mod_factors['ipc_scale'][trace_name] = mod_factors['ipc'][trace_name] / mod_factors['ipc'][first_trace] * 100.0\n",
    "    except Exception as e:\n",
    "        mod_factors['ipc_scale'][trace_name] = 'NaN'\n",
    "    try: #except NaN\n",
    "        mod_factors['freq'][trace_name] = float(raw_data['useful_cyc'][trace_name]) / float(raw_data['useful_tot'][trace_name]) / 1000\n",
    "    except:\n",
    "        mod_factors['freq'][trace_name] = 'NaN'\n",
    "    try: #except NaN\n",
    "        freq_first_trace = float(raw_data['useful_cyc'][first_trace]) / float(raw_data['useful_tot'][first_trace]) / 1000\n",
    "        mod_factors['freq_scale'][trace_name] = mod_factors['freq'][trace_name] / freq_first_trace * 100.0\n",
    "        # Do not reuse mod_factors for the first_trace to avoid INOUT\n",
    "        # mod_factors['freq_scale'][trace_name] = mod_factors['freq'][trace_name] / mod_factors['freq'][first_trace] * 100.0\n",
    "    except Exception as e:\n",
    "        mod_factors['freq_scale'][trace_name] = 'NaN'\n",
    "    try: #except NaN\n",
    "        if scaling == 'strong':\n",
    "            mod_factors['inst_scale'][trace_name] = float(raw_data['useful_ins'][first_trace]) / float(raw_data['useful_ins'][trace_name]) * 100.0\n",
    "        else:\n",
    "            mod_factors['inst_scale'][trace_name] = float(raw_data['useful_ins'][first_trace]) / float(raw_data['useful_ins'][trace_name]) * proc_ratio * 100.0\n",
    "    except:\n",
    "        mod_factors['inst_scale'][trace_name] = 'NaN'\n",
    "    try: #except NaN\n",
    "        if scaling == 'strong':\n",
    "            mod_factors['speedup'][trace_name] = raw_data['runtime'][first_trace] / raw_data['runtime'][trace_name]\n",
    "        else:\n",
    "            mod_factors['speedup'][trace_name] = raw_data['runtime'][first_trace] / raw_data['runtime'][trace_name] * proc_ratio\n",
    "    except:\n",
    "        mod_factors['speedup'][trace_name] = 'NaN'\n",
    "\n",
    "    return mod_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(returns=(str, str, str))\n",
    "def fit_amdahl(x_proc, load_opt, comm_opt, comp_opt):\n",
    "    load_fit = ' '.join(['load( x ) = ( x >',str(x_proc[0]),') ?',str(load_opt[0]),'/ (',str(load_opt[1]),'+ ( 1 -',str(load_opt[1]),') * x ) : 1/0'])\n",
    "    comm_fit = ' '.join(['comm( x ) = ( x >',str(x_proc[0]),') ?',str(comm_opt[0]),'/ (',str(comm_opt[1]),'+ ( 1 -',str(comm_opt[1]),') * x ) : 1/0'])\n",
    "    comp_fit = ' '.join(['comp( x ) = ( x >',str(x_proc[0]),') ?',str(comp_opt[0]),'/ (',str(comp_opt[1]),'+ ( 1 -',str(comp_opt[1]),') * x ) : 1/0'])\n",
    "    return load_fit, comm_fit, comp_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(returns=(str, str, str))\n",
    "def fit_pipe(x_proc, load_opt, comm_opt, comp_opt):\n",
    "    load_fit = ' '.join(['load( x ) = ( x >', str(x_proc[0]),') ?', str(load_opt[0]),'* x / ( ( 1 -', str(load_opt[1]),') +', str(load_opt[1]),'* ( 2 * x - 1 ) ) : 1/0'])\n",
    "    comm_fit = ' '.join(['comm( x ) = ( x >', str(x_proc[0]),') ?', str(comm_opt[0]),'* x / ( ( 1 -', str(comm_opt[1]),') +', str(comm_opt[1]),'* ( 2 * x - 1 ) ) : 1/0'])\n",
    "    comp_fit = ' '.join(['comp( x ) = ( x >', str(x_proc[0]),') ?', str(comp_opt[0]),'* x / ( ( 1 -', str(comp_opt[1]),') +', str(comp_opt[1]),'* ( 2 * x - 1 ) ) : 1/0'])\n",
    "    return load_fit, comm_fit, comp_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(returns=(str, str, str))\n",
    "def fit_linear(x_proc, load_opt, comm_opt, comp_opt):\n",
    "    load_fit = ' '.join(['load( x ) = ( x >', str(x_proc[0]), ') ?', str(load_opt[0]), '+ x *', str(load_opt[1]), ': 1/0'])\n",
    "    comm_fit = ' '.join(['comm( x ) = ( x >', str(x_proc[0]), ') ?', str(comm_opt[0]), '+ x *', str(comm_opt[1]), ': 1/0'])\n",
    "    comp_fit = ' '.join(['comp( x ) = ( x >', str(x_proc[0]), ') ?', str(comp_opt[0]), '+ x *', str(comp_opt[1]), ': 1/0'])\n",
    "    return load_fit, comm_fit, comp_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(file_path=FILE_OUT)\n",
    "def create_gnuplot(limit, para_fit, load_fit, comm_fit, comp_fit, glob_fit, points_data, cfgs_path, file_path):\n",
    "    #Create Gnuplot file\n",
    "    gp_template = os.path.join(cfgs_path, 'modelfactors.gp')\n",
    "    content = []\n",
    "    with open(gp_template) as f:\n",
    "        content = f.readlines()\n",
    "     \n",
    "    #Replace xrange\n",
    "    content = [line.replace('#REPLACE_BY_XRANGE', ''.join(['set xrange [1:',limit,']']) ) for line in content]\n",
    "\n",
    "    #Replace projection functions\n",
    "    content = [line.replace('#REPLACE_BY_PARA_FUNCTION', para_fit ) for line in content]\n",
    "    content = [line.replace('#REPLACE_BY_LOAD_FUNCTION', load_fit ) for line in content]\n",
    "    content = [line.replace('#REPLACE_BY_COMM_FUNCTION', comm_fit ) for line in content]\n",
    "    content = [line.replace('#REPLACE_BY_COMP_FUNCTION', comp_fit ) for line in content]\n",
    "    content = [line.replace('#REPLACE_BY_GLOB_FUNCTION', glob_fit ) for line in content]\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.writelines(content)\n",
    "        \n",
    "    x_proc, y_para, y_load, y_comm, y_comp, y_glob, number_traces = points_data\n",
    "        \n",
    "    #Add data points to gnuplot file\n",
    "    with open(file_path, 'a') as f:\n",
    "        for index in range(0, number_traces):\n",
    "            line = ' '.join([str(x_proc[index]), str(y_para[index]), '\\n'])\n",
    "            f.write(line)\n",
    "        f.write('e\\n')\n",
    "\n",
    "        for index in range(0, number_traces):\n",
    "            line = ' '.join([str(x_proc[index]), str(y_load[index]), '\\n'])\n",
    "            f.write(line)\n",
    "        f.write('e\\n')\n",
    "\n",
    "        for index in range(0, number_traces):\n",
    "            line = ' '.join([str(x_proc[index]), str(y_comm[index]), '\\n'])\n",
    "            f.write(line)\n",
    "        f.write('e\\n')\n",
    "\n",
    "        for index in range(0, number_traces):\n",
    "            line = ' '.join([str(x_proc[index]), str(y_comp[index]), '\\n'])\n",
    "            f.write(line)\n",
    "        f.write('e\\n')\n",
    "\n",
    "        for index in range(0, number_traces):\n",
    "            line = ' '.join([str(x_proc[index]), str(y_glob[index]), '\\n'])\n",
    "            f.write(line)\n",
    "        f.write('e\\n')\n",
    "\n",
    "        f.write('\\n')\n",
    "        f.write('pause -1\\n')\n",
    "\n",
    "    print('Projection written to ' + file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed with gnuplot generation\n",
    "# @task(file_path=FILE_INOUT)\n",
    "# def write_projection(x_proc, y_para, y_load, y_comm, y_comp, y_glob, number_traces, file_path):\n",
    "#     #Add data points to gnuplot file\n",
    "#     with open(file_path, 'a') as f:\n",
    "#         for index in range(0, number_traces):\n",
    "#             line = ' '.join([str(x_proc[index]), str(y_para[index]), '\\n'])\n",
    "#             f.write(line)\n",
    "#         f.write('e\\n')\n",
    "\n",
    "#         for index in range(0, number_traces):\n",
    "#             line = ' '.join([str(x_proc[index]), str(y_load[index]), '\\n'])\n",
    "#             f.write(line)\n",
    "#         f.write('e\\n')\n",
    "\n",
    "#         for index in range(0, number_traces):\n",
    "#             line = ' '.join([str(x_proc[index]), str(y_comm[index]), '\\n'])\n",
    "#             f.write(line)\n",
    "#         f.write('e\\n')\n",
    "\n",
    "#         for index in range(0, number_traces):\n",
    "#             line = ' '.join([str(x_proc[index]), str(y_comp[index]), '\\n'])\n",
    "#             f.write(line)\n",
    "#         f.write('e\\n')\n",
    "\n",
    "#         for index in range(0, number_traces):\n",
    "#             line = ' '.join([str(x_proc[index]), str(y_glob[index]), '\\n'])\n",
    "#             f.write(line)\n",
    "#         f.write('e\\n')\n",
    "\n",
    "#         f.write('\\n')\n",
    "#         f.write('pause -1\\n')\n",
    "\n",
    "#     print('Projection written to ' + file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(file_path=FILE_OUT)\n",
    "def create_matplotlib(limit, para_fit, load_fit, comm_fit, comp_fit, glob_fit, points_data, cfgs_path, file_path):\n",
    "    x_proc, y_para, y_load, y_comm, y_comp, y_glob, number_traces = points_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "\n",
    "In the following cell, the necessary widgets for interactive executions are defined as well as the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import os\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "class wdgts(object):\n",
    "    # list of traces to process. Accepts wild cards and automatically filters for valid traces\n",
    "    w_trace_folder = widgets.Text(value=os.getcwd() + os.path.sep + 'traces/gromacs_jesus/',\n",
    "                                  description='List of traces:',\n",
    "                                  layout={'width':'60%'})\n",
    "    # increase output verbosity to debug level\n",
    "    w_debug = widgets.Checkbox(value=False,\n",
    "                               description='Debug')\n",
    "    # define whether the measurements are weak or strong scaling (default: auto)\n",
    "    w_scaling = widgets.ToggleButtons(options=['auto', 'weak','strong'],\n",
    "                                      description='Scaling',\n",
    "                                      button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "                                      tooltips=['Automatic measurements scaling', 'weak measurements scaling', 'Strong measurements scaling'])\n",
    "    # run only the projection for the given modelfactors.csv (default: false)\n",
    "    w_project = widgets.Text(value='false',\n",
    "                             placeholder='modelfactors.csv',\n",
    "                             description='CSV projection file path:',\n",
    "                             style=style,\n",
    "                             layout={'width':'60%'})\n",
    "    # limit number of cores for the projection (default: 10000)\n",
    "    w_limit = widgets.IntText(value=10000,\n",
    "                              description='Projection # cores:',\n",
    "                              style=style,\n",
    "                              layout={'width':'60%'})\n",
    "    # select model for prediction (default: amdahl)\n",
    "    w_model = widgets.ToggleButtons(options=['amdahl','pipe','linear'],\n",
    "                                    description='Model',\n",
    "                                    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "                                    tooltips=['Amdahl model prediction', 'Pipe model prediction', 'Linear model prediction'])\n",
    "    # set bounds for the prediction (default: yes)\n",
    "    w_bounds = widgets.Checkbox(value=True,\n",
    "                                description='Prediction bounds')\n",
    "    # set error restrains for prediction (default: first). first: prioritize smallest run; equal: no priority; decrease: decreasing priority for larger runs\n",
    "    w_sigma = widgets.ToggleButtons(options=['first','equal','decrease'],\n",
    "                                    description='Sigma',\n",
    "                                    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "                                    tooltips=['Prioritize smallest run', 'No priority', 'Decreasing priority for larger runs'])\n",
    "    # path of the configuration files\n",
    "    w_cfgs = widgets.Text(value=os.getcwd() + os.path.sep + 'cfgs',\n",
    "                          placeholder='cfgs',\n",
    "                          description='Configuration files path:',\n",
    "                          style=style,\n",
    "                         layout={'width':'60%'})\n",
    "    # path of output file\n",
    "    w_out = widgets.Text(value='results.out',\n",
    "                         placeholder='Output_file.out',\n",
    "                         description='Output file:',\n",
    "                         style=style,\n",
    "                         layout={'width':'60%'})\n",
    "    # path of csv output file\n",
    "    w_csv = widgets.Text(value='results.csv',\n",
    "                         placeholder='Output_file.csv',\n",
    "                         description='CSV output file:',\n",
    "                         style=style,\n",
    "                         layout={'width':'60%'})\n",
    "\n",
    "\n",
    "def model_factors(trace_folder, debug, scaling, project, limit, model, bounds, sigma, cfgs, out, csv):\n",
    "    \"\"\"Main control flow.\n",
    "    Currently the script only accepts one parameter, which is a list of traces\n",
    "    that are processed. This can be a regex with wild cards and only valid trace\n",
    "    files are kept at the end.\n",
    "    \"\"\"\n",
    "    trace_list = []\n",
    "    for file in os.listdir(trace_folder):\n",
    "        if file.endswith(\".prv\"):\n",
    "            trace_list.append(os.path.join(trace_folder, file))\n",
    "            \n",
    "    if debug:\n",
    "        print(\"Traces  :\")\n",
    "        for t in trace_list:\n",
    "            print(\"\\t- \" + str(t))\n",
    "        print(\"Debug   : \" + str(debug))\n",
    "        print(\"Scaling : \" + str(scaling))\n",
    "        print(\"Project : \" + str(project))\n",
    "        print(\"Limit   : \" + str(limit))\n",
    "        print(\"Model   : \" + str(model))\n",
    "        print(\"Bounds  : \" + str(bounds))\n",
    "        print(\"Sigma   : \" + str(sigma))\n",
    "        print(\"Cfgs    : \" + str(cfgs))\n",
    "        print(\"Out     : \" + str(out))\n",
    "        print(\"Csv     : \" + str(csv))\n",
    "        \n",
    "        \n",
    "    # Parse command line arguments\n",
    "    # cmdl_args = parse_arguments() They have already been parsed by the widgets... so remove all cmdl_args\n",
    "    out = os.path.abspath(out)\n",
    "    cfgs = os.path.abspath(cfgs)\n",
    "    # Check if paramedir and Dimemas are in the path\n",
    "    check_installation(debug) ## TODO: CHECK INSTALLATION OF DIMEMAS\n",
    "    # Check if projection-only mode is selected\n",
    "    # If not: compute everything\n",
    "    # Else: read the passed modelfactors.csv\n",
    "    if project == 'false':\n",
    "        # trace_list, trace_processes = get_traces_from_args(trace_list)\n",
    "        traces = get_traces_from_args(trace_list)\n",
    "        \n",
    "        lraw_data = []\n",
    "        timings       = os.path.join(cfgs, 'timings.cfg')\n",
    "        runtime       = os.path.join(cfgs, 'runtime.cfg')\n",
    "        cycles        = os.path.join(cfgs, 'cycles.cfg')\n",
    "        inst  = os.path.join(cfgs, 'instructions.cfg')\n",
    "        dimemas_cfgs = os.path.join(cfgs, 'dimemas_ideal.cfg')\n",
    "        dimemas_collectives = os.path.join(cfgs, 'dimemas.collectives')\n",
    "\n",
    "        for name, trace in traces.items():\n",
    "            partial_raw_data = gather_raw_data(trace.get_path(), timings, runtime, cycles, inst, dimemas_cfgs, dimemas_collectives, trace.get_processes(), cfgs, debug)\n",
    "            lraw_data.append(partial_raw_data)\n",
    "        raw_data = reduce(merge_dicts, lraw_data)\n",
    "        \n",
    "        # Guess the weak or strong scaling\n",
    "        scaling = get_scaling_type(raw_data, traces, scaling, debug)\n",
    "\n",
    "        # Compute the model factors and print them\n",
    "        lmod_factors = []\n",
    "        first_trace = None\n",
    "        first_processes = None\n",
    "        for name, trace in traces.items():\n",
    "            if first_processes is None and first_trace is None:\n",
    "                first_trace = name\n",
    "                first_processes = trace.get_processes()\n",
    "            partial_mod_factors = compute_model_factors(raw_data, trace.get_path(), trace.get_processes(), first_trace, first_processes, scaling, debug)\n",
    "            lmod_factors.append(partial_mod_factors) \n",
    "        mod_factors = reduce(merge_dicts, lmod_factors)\n",
    "        \n",
    "        mod_factors = compss_wait_on(mod_factors)\n",
    "        raw_data = compss_wait_on(raw_data)\n",
    "        print_raw_data_table_html(raw_data, traces)        # Remove _html for normal print\n",
    "        print_mod_factors_table_html(mod_factors, traces)  # Remove _html for normal print\n",
    "        print_mod_factors_csv(mod_factors, raw_data, traces, csv)\n",
    "    else:\n",
    "        # Read the model factors from the csv file\n",
    "        mod_factors, trace_list, trace_processes = read_mod_factors_csv(debug, project)\n",
    "\n",
    "    print(\"Compute_projection\")\n",
    "    compute_projection(mod_factors, traces, debug, model, limit, bounds, sigma, out, cfgs)\n",
    "    \n",
    "widgets.interact_manual(model_factors, trace_folder=wdgts.w_trace_folder, debug=wdgts.w_debug, scaling=wdgts.w_scaling, project=wdgts.w_project, limit=wdgts.w_limit, model=wdgts.w_model, bounds=wdgts.w_bounds, sigma=wdgts.w_sigma, cfgs=wdgts.w_cfgs, out=wdgts.w_out, csv=wdgts.w_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipycompss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_raw = ['Parameter', 24, 48, 96, 192]\n",
    "raw = [['Runtime (us)', 16722763.02, 8754638.15, 4714094.74, 2285913.92],\n",
    "       ['Runtime (ideal)', 16495194.04, 8542198.57, 4628710.67, 2199460.51],\n",
    "       ['Useful duration (average)', 15171967.02, 7698566.41, 3951799.18, 1852709.09],\n",
    "       ['Useful duration (maximum)', 16380967.8, 8429582.16, 4516863.01, 2132687.88],\n",
    "       ['Useful duration (total)', 364127208.59, 369531187.88, 379372720.92, 355720145.85],\n",
    "       ['Useful duration (ideal, max)', 16380967.8, 8429582.16, 4516863.01, 2132687.88],\n",
    "       ['Useful instructions (total)', 1089905844835, 1128635398017, 1215612183046, 1167055295613],\n",
    "       ['Useful cycles (total)', 990700240272, 1000470887596, 1041832237966, 976609105921]]\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "display(HTML(tabulate.tabulate(raw, headers=titles_raw, tablefmt='html', floatfmt=\".2f\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
